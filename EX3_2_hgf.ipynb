{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import tensorflow as tf\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "import plotly.express as px\r\n",
    "\r\n",
    "from sklearn import datasets\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn.compose import ColumnTransformer\r\n",
    "\r\n",
    "from keras.models import Sequential, load_model\r\n",
    "from keras.layers import Dense\r\n",
    "from keras.layers import Dropout\r\n",
    "from keras.utils.vis_utils import plot_model\r\n",
    "from keras.utils.np_utils import  to_categorical\r\n",
    "from keras.callbacks import ModelCheckpoint\r\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\r\n",
    "\r\n",
    "\r\n",
    "# Ignorar Avisos desnecessários\r\n",
    "import warnings\r\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\r\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confere se temos GPU instalada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n",
    "#print(tf.config.experimental.list_physical_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\r\n",
    "    acc = history.history['accuracy']\r\n",
    "    val_acc = history.history['val_accuracy']\r\n",
    "    loss = history.history['loss']\r\n",
    "    val_loss = history.history['val_loss']\r\n",
    "    x = range(1, len(acc) + 1)\r\n",
    "\r\n",
    "    plt.figure(figsize=(12, 5))\r\n",
    "    plt.subplot(1, 2, 1)\r\n",
    "    plt.plot(x, acc, 'b', label='Training Accuracy')\r\n",
    "    plt.plot(x, val_acc, 'r', label='Validation Accuracy')\r\n",
    "    plt.title('Training and validation Accuracy')\r\n",
    "    plt.subplot(1, 2, 2)\r\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\r\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\r\n",
    "    plt.title('Training and validation loss')\r\n",
    "    plt.legend()\r\n",
    "    \r\n",
    "def min_max(serie):\r\n",
    "    min = serie.min()\r\n",
    "    max = serie.max()\r\n",
    "    return serie.apply(lambda x: (x-min)/(max-min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise dos Dados\r\n",
    "\r\n",
    "## Carregando a base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/heart.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       303 non-null    int64  \n",
      " 1   sex       303 non-null    int64  \n",
      " 2   cp        303 non-null    int64  \n",
      " 3   trestbps  303 non-null    int64  \n",
      " 4   chol      303 non-null    int64  \n",
      " 5   fbs       303 non-null    int64  \n",
      " 6   restecg   303 non-null    int64  \n",
      " 7   thalach   303 non-null    int64  \n",
      " 8   exang     303 non-null    int64  \n",
      " 9   oldpeak   303 non-null    float64\n",
      " 10  slope     303 non-null    int64  \n",
      " 11  ca        303 non-null    int64  \n",
      " 12  thal      303 non-null    int64  \n",
      " 13  target    303 non-null    int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 33.3 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>54.366337</td>\n",
       "      <td>0.683168</td>\n",
       "      <td>0.966997</td>\n",
       "      <td>131.623762</td>\n",
       "      <td>246.264026</td>\n",
       "      <td>0.148515</td>\n",
       "      <td>0.528053</td>\n",
       "      <td>149.646865</td>\n",
       "      <td>0.326733</td>\n",
       "      <td>1.039604</td>\n",
       "      <td>1.399340</td>\n",
       "      <td>0.729373</td>\n",
       "      <td>2.313531</td>\n",
       "      <td>0.544554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.082101</td>\n",
       "      <td>0.466011</td>\n",
       "      <td>1.032052</td>\n",
       "      <td>17.538143</td>\n",
       "      <td>51.830751</td>\n",
       "      <td>0.356198</td>\n",
       "      <td>0.525860</td>\n",
       "      <td>22.905161</td>\n",
       "      <td>0.469794</td>\n",
       "      <td>1.161075</td>\n",
       "      <td>0.616226</td>\n",
       "      <td>1.022606</td>\n",
       "      <td>0.612277</td>\n",
       "      <td>0.498835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>47.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>274.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age         sex          cp    trestbps        chol         fbs  \\\n",
       "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
       "mean    54.366337    0.683168    0.966997  131.623762  246.264026    0.148515   \n",
       "std      9.082101    0.466011    1.032052   17.538143   51.830751    0.356198   \n",
       "min     29.000000    0.000000    0.000000   94.000000  126.000000    0.000000   \n",
       "25%     47.500000    0.000000    0.000000  120.000000  211.000000    0.000000   \n",
       "50%     55.000000    1.000000    1.000000  130.000000  240.000000    0.000000   \n",
       "75%     61.000000    1.000000    2.000000  140.000000  274.500000    0.000000   \n",
       "max     77.000000    1.000000    3.000000  200.000000  564.000000    1.000000   \n",
       "\n",
       "          restecg     thalach       exang     oldpeak       slope          ca  \\\n",
       "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
       "mean     0.528053  149.646865    0.326733    1.039604    1.399340    0.729373   \n",
       "std      0.525860   22.905161    0.469794    1.161075    0.616226    1.022606   \n",
       "min      0.000000   71.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000  133.500000    0.000000    0.000000    1.000000    0.000000   \n",
       "50%      1.000000  153.000000    0.000000    0.800000    1.000000    0.000000   \n",
       "75%      1.000000  166.000000    1.000000    1.600000    2.000000    1.000000   \n",
       "max      2.000000  202.000000    1.000000    6.200000    2.000000    4.000000   \n",
       "\n",
       "             thal      target  \n",
       "count  303.000000  303.000000  \n",
       "mean     2.313531    0.544554  \n",
       "std      0.612277    0.498835  \n",
       "min      0.000000    0.000000  \n",
       "25%      2.000000    0.000000  \n",
       "50%      2.000000    1.000000  \n",
       "75%      3.000000    1.000000  \n",
       "max      3.000000    1.000000  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    45.54\n",
       "1    54.46\n",
       "Name: age, dtype: float64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(df.groupby('target').count().age / df.shape[0] * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df.target)\r\n",
    "plt.title('Distribuição da variável Target')\r\n",
    "plt.xlabel('Target')\r\n",
    "plt.xticks(ticks=range(0,2), labels=['0-no disease', '1-heart disease'])\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "diagonal": {
          "visible": false
         },
         "dimensions": [
          {
           "axis": {
            "matches": true
           },
           "label": "age",
           "values": [
            63,
            37,
            41,
            56,
            57,
            57,
            56,
            44,
            52,
            57,
            54,
            48,
            49,
            64,
            58,
            50,
            58,
            66,
            43,
            69,
            59,
            44,
            42,
            61,
            40,
            71,
            59,
            51,
            65,
            53,
            41,
            65,
            44,
            54,
            51,
            46,
            54,
            54,
            65,
            65,
            51,
            48,
            45,
            53,
            39,
            52,
            44,
            47,
            53,
            53,
            51,
            66,
            62,
            44,
            63,
            52,
            48,
            45,
            34,
            57,
            71,
            54,
            52,
            41,
            58,
            35,
            51,
            45,
            44,
            62,
            54,
            51,
            29,
            51,
            43,
            55,
            51,
            59,
            52,
            58,
            41,
            45,
            60,
            52,
            42,
            67,
            68,
            46,
            54,
            58,
            48,
            57,
            52,
            54,
            45,
            53,
            62,
            52,
            43,
            53,
            42,
            59,
            63,
            42,
            50,
            68,
            69,
            45,
            50,
            50,
            64,
            57,
            64,
            43,
            55,
            37,
            41,
            56,
            46,
            46,
            64,
            59,
            41,
            54,
            39,
            34,
            47,
            67,
            52,
            74,
            54,
            49,
            42,
            41,
            41,
            49,
            60,
            62,
            57,
            64,
            51,
            43,
            42,
            67,
            76,
            70,
            44,
            60,
            44,
            42,
            66,
            71,
            64,
            66,
            39,
            58,
            47,
            35,
            58,
            56,
            56,
            55,
            41,
            38,
            38,
            67,
            67,
            62,
            63,
            53,
            56,
            48,
            58,
            58,
            60,
            40,
            60,
            64,
            43,
            57,
            55,
            65,
            61,
            58,
            50,
            44,
            60,
            54,
            50,
            41,
            51,
            58,
            54,
            60,
            60,
            59,
            46,
            67,
            62,
            65,
            44,
            60,
            58,
            68,
            62,
            52,
            59,
            60,
            49,
            59,
            57,
            61,
            39,
            61,
            56,
            43,
            62,
            63,
            65,
            48,
            63,
            55,
            65,
            56,
            54,
            70,
            62,
            35,
            59,
            64,
            47,
            57,
            55,
            64,
            70,
            51,
            58,
            60,
            77,
            35,
            70,
            59,
            64,
            57,
            56,
            48,
            56,
            66,
            54,
            69,
            51,
            43,
            62,
            67,
            59,
            45,
            58,
            50,
            62,
            38,
            66,
            52,
            53,
            63,
            54,
            66,
            55,
            49,
            54,
            56,
            46,
            61,
            67,
            58,
            47,
            52,
            58,
            57,
            58,
            61,
            42,
            52,
            59,
            40,
            61,
            46,
            59,
            57,
            57,
            55,
            61,
            58,
            58,
            67,
            44,
            63,
            63,
            59,
            57,
            45,
            68,
            57,
            57
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "sex",
           "values": [
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "cp",
           "values": [
            3,
            2,
            1,
            1,
            0,
            0,
            1,
            1,
            2,
            2,
            0,
            2,
            1,
            3,
            3,
            2,
            2,
            3,
            0,
            3,
            0,
            2,
            0,
            2,
            3,
            1,
            2,
            2,
            2,
            2,
            1,
            0,
            1,
            2,
            3,
            2,
            2,
            2,
            2,
            2,
            2,
            1,
            0,
            0,
            2,
            1,
            2,
            2,
            2,
            0,
            2,
            0,
            2,
            2,
            2,
            1,
            0,
            0,
            3,
            0,
            2,
            1,
            3,
            1,
            2,
            0,
            2,
            1,
            1,
            0,
            2,
            2,
            1,
            0,
            2,
            1,
            2,
            1,
            1,
            2,
            2,
            1,
            2,
            3,
            0,
            2,
            2,
            1,
            2,
            0,
            2,
            0,
            2,
            1,
            1,
            0,
            0,
            0,
            2,
            2,
            3,
            3,
            1,
            2,
            2,
            2,
            3,
            0,
            1,
            0,
            0,
            2,
            2,
            0,
            1,
            2,
            2,
            3,
            1,
            0,
            0,
            0,
            2,
            2,
            2,
            1,
            0,
            2,
            2,
            1,
            2,
            1,
            1,
            1,
            1,
            0,
            2,
            1,
            0,
            0,
            2,
            0,
            2,
            0,
            2,
            1,
            2,
            3,
            2,
            2,
            0,
            0,
            3,
            2,
            2,
            0,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            0,
            0,
            0,
            0,
            0,
            2,
            1,
            1,
            2,
            0,
            0,
            0,
            2,
            0,
            0,
            0,
            0,
            0,
            2,
            0,
            0,
            0,
            0,
            2,
            0,
            0,
            0,
            0,
            0,
            2,
            0,
            2,
            0,
            0,
            0,
            0,
            0,
            0,
            2,
            0,
            0,
            0,
            0,
            2,
            0,
            2,
            0,
            0,
            0,
            0,
            0,
            2,
            0,
            0,
            0,
            0,
            0,
            3,
            0,
            0,
            0,
            1,
            0,
            3,
            2,
            2,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            2,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            2,
            0,
            0,
            0,
            0,
            3,
            0,
            0,
            0,
            0,
            3,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            2,
            0,
            0,
            0,
            3,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            2,
            0,
            0,
            0,
            3,
            1,
            0,
            0,
            0,
            0,
            0,
            2,
            0,
            0,
            0,
            0,
            0,
            3,
            0,
            0,
            1
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "trestbps",
           "values": [
            145,
            130,
            130,
            120,
            120,
            140,
            140,
            120,
            172,
            150,
            140,
            130,
            130,
            110,
            150,
            120,
            120,
            150,
            150,
            140,
            135,
            130,
            140,
            150,
            140,
            160,
            150,
            110,
            140,
            130,
            105,
            120,
            130,
            125,
            125,
            142,
            135,
            150,
            155,
            160,
            140,
            130,
            104,
            130,
            140,
            120,
            140,
            138,
            128,
            138,
            130,
            120,
            130,
            108,
            135,
            134,
            122,
            115,
            118,
            128,
            110,
            108,
            118,
            135,
            140,
            138,
            100,
            130,
            120,
            124,
            120,
            94,
            130,
            140,
            122,
            135,
            125,
            140,
            128,
            105,
            112,
            128,
            102,
            152,
            102,
            115,
            118,
            101,
            110,
            100,
            124,
            132,
            138,
            132,
            112,
            142,
            140,
            108,
            130,
            130,
            148,
            178,
            140,
            120,
            129,
            120,
            160,
            138,
            120,
            110,
            180,
            150,
            140,
            110,
            130,
            120,
            130,
            120,
            105,
            138,
            130,
            138,
            112,
            108,
            94,
            118,
            112,
            152,
            136,
            120,
            160,
            134,
            120,
            110,
            126,
            130,
            120,
            128,
            110,
            128,
            120,
            115,
            120,
            106,
            140,
            156,
            118,
            150,
            120,
            130,
            160,
            112,
            170,
            146,
            138,
            130,
            130,
            122,
            125,
            130,
            120,
            132,
            120,
            138,
            138,
            160,
            120,
            140,
            130,
            140,
            130,
            110,
            120,
            132,
            130,
            110,
            117,
            140,
            120,
            150,
            132,
            150,
            130,
            112,
            150,
            112,
            130,
            124,
            140,
            110,
            130,
            128,
            120,
            145,
            140,
            170,
            150,
            125,
            120,
            110,
            110,
            125,
            150,
            180,
            160,
            128,
            110,
            150,
            120,
            140,
            128,
            120,
            118,
            145,
            125,
            132,
            130,
            130,
            135,
            130,
            150,
            140,
            138,
            200,
            110,
            145,
            120,
            120,
            170,
            125,
            108,
            165,
            160,
            120,
            130,
            140,
            125,
            140,
            125,
            126,
            160,
            174,
            145,
            152,
            132,
            124,
            134,
            160,
            192,
            140,
            140,
            132,
            138,
            100,
            160,
            142,
            128,
            144,
            150,
            120,
            178,
            112,
            123,
            108,
            110,
            112,
            180,
            118,
            122,
            130,
            120,
            134,
            120,
            100,
            110,
            125,
            146,
            124,
            136,
            138,
            136,
            128,
            126,
            152,
            140,
            140,
            134,
            154,
            110,
            128,
            148,
            114,
            170,
            152,
            120,
            140,
            124,
            164,
            140,
            110,
            144,
            130,
            130
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "chol",
           "values": [
            233,
            250,
            204,
            236,
            354,
            192,
            294,
            263,
            199,
            168,
            239,
            275,
            266,
            211,
            283,
            219,
            340,
            226,
            247,
            239,
            234,
            233,
            226,
            243,
            199,
            302,
            212,
            175,
            417,
            197,
            198,
            177,
            219,
            273,
            213,
            177,
            304,
            232,
            269,
            360,
            308,
            245,
            208,
            264,
            321,
            325,
            235,
            257,
            216,
            234,
            256,
            302,
            231,
            141,
            252,
            201,
            222,
            260,
            182,
            303,
            265,
            309,
            186,
            203,
            211,
            183,
            222,
            234,
            220,
            209,
            258,
            227,
            204,
            261,
            213,
            250,
            245,
            221,
            205,
            240,
            250,
            308,
            318,
            298,
            265,
            564,
            277,
            197,
            214,
            248,
            255,
            207,
            223,
            288,
            160,
            226,
            394,
            233,
            315,
            246,
            244,
            270,
            195,
            240,
            196,
            211,
            234,
            236,
            244,
            254,
            325,
            126,
            313,
            211,
            262,
            215,
            214,
            193,
            204,
            243,
            303,
            271,
            268,
            267,
            199,
            210,
            204,
            277,
            196,
            269,
            201,
            271,
            295,
            235,
            306,
            269,
            178,
            208,
            201,
            263,
            295,
            303,
            209,
            223,
            197,
            245,
            242,
            240,
            226,
            180,
            228,
            149,
            227,
            278,
            220,
            197,
            253,
            192,
            220,
            221,
            240,
            342,
            157,
            175,
            175,
            286,
            229,
            268,
            254,
            203,
            256,
            229,
            284,
            224,
            206,
            167,
            230,
            335,
            177,
            276,
            353,
            225,
            330,
            230,
            243,
            290,
            253,
            266,
            233,
            172,
            305,
            216,
            188,
            282,
            185,
            326,
            231,
            254,
            267,
            248,
            197,
            258,
            270,
            274,
            164,
            255,
            239,
            258,
            188,
            177,
            229,
            260,
            219,
            307,
            249,
            341,
            263,
            330,
            254,
            256,
            407,
            217,
            282,
            288,
            239,
            174,
            281,
            198,
            288,
            309,
            243,
            289,
            289,
            246,
            322,
            299,
            300,
            293,
            304,
            282,
            269,
            249,
            212,
            274,
            184,
            274,
            409,
            246,
            283,
            254,
            298,
            247,
            294,
            299,
            273,
            309,
            259,
            200,
            244,
            231,
            228,
            230,
            282,
            269,
            206,
            212,
            327,
            149,
            286,
            283,
            249,
            234,
            237,
            234,
            275,
            212,
            218,
            261,
            319,
            166,
            315,
            204,
            218,
            223,
            207,
            311,
            204,
            232,
            335,
            205,
            203,
            318,
            225,
            212,
            169,
            187,
            197,
            176,
            241,
            264,
            193,
            131,
            236
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "fbs",
           "values": [
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "restecg",
           "values": [
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            2,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            2,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            2,
            1,
            2,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "thalach",
           "values": [
            150,
            187,
            172,
            178,
            163,
            148,
            153,
            173,
            162,
            174,
            160,
            139,
            171,
            144,
            162,
            158,
            172,
            114,
            171,
            151,
            161,
            179,
            178,
            137,
            178,
            162,
            157,
            123,
            157,
            152,
            168,
            140,
            188,
            152,
            125,
            160,
            170,
            165,
            148,
            151,
            142,
            180,
            148,
            143,
            182,
            172,
            180,
            156,
            115,
            160,
            149,
            151,
            146,
            175,
            172,
            158,
            186,
            185,
            174,
            159,
            130,
            156,
            190,
            132,
            165,
            182,
            143,
            175,
            170,
            163,
            147,
            154,
            202,
            186,
            165,
            161,
            166,
            164,
            184,
            154,
            179,
            170,
            160,
            178,
            122,
            160,
            151,
            156,
            158,
            122,
            175,
            168,
            169,
            159,
            138,
            111,
            157,
            147,
            162,
            173,
            178,
            145,
            179,
            194,
            163,
            115,
            131,
            152,
            162,
            159,
            154,
            173,
            133,
            161,
            155,
            170,
            168,
            162,
            172,
            152,
            122,
            182,
            172,
            167,
            179,
            192,
            143,
            172,
            169,
            121,
            163,
            162,
            162,
            153,
            163,
            163,
            96,
            140,
            126,
            105,
            157,
            181,
            173,
            142,
            116,
            143,
            149,
            171,
            169,
            150,
            138,
            125,
            155,
            152,
            152,
            131,
            179,
            174,
            144,
            163,
            169,
            166,
            182,
            173,
            173,
            108,
            129,
            160,
            147,
            155,
            142,
            168,
            160,
            173,
            132,
            114,
            160,
            158,
            120,
            112,
            132,
            114,
            169,
            165,
            128,
            153,
            144,
            109,
            163,
            158,
            142,
            131,
            113,
            142,
            155,
            140,
            147,
            163,
            99,
            158,
            177,
            141,
            111,
            150,
            145,
            161,
            142,
            157,
            139,
            162,
            150,
            140,
            140,
            146,
            144,
            136,
            97,
            132,
            127,
            150,
            154,
            111,
            174,
            133,
            126,
            125,
            103,
            130,
            159,
            131,
            152,
            124,
            145,
            96,
            109,
            173,
            171,
            170,
            162,
            156,
            112,
            143,
            132,
            88,
            105,
            166,
            150,
            120,
            195,
            146,
            122,
            143,
            106,
            125,
            125,
            147,
            130,
            126,
            154,
            182,
            165,
            160,
            95,
            169,
            108,
            132,
            117,
            126,
            116,
            103,
            144,
            145,
            71,
            156,
            118,
            168,
            105,
            141,
            152,
            125,
            125,
            156,
            134,
            181,
            138,
            120,
            162,
            164,
            143,
            130,
            161,
            140,
            146,
            150,
            144,
            144,
            136,
            90,
            123,
            132,
            141,
            115,
            174
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "exang",
           "values": [
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "oldpeak",
           "values": [
            2.3,
            3.5,
            1.4,
            0.8,
            0.6,
            0.4,
            1.3,
            0,
            0.5,
            1.6,
            1.2,
            0.2,
            0.6,
            1.8,
            1,
            1.6,
            0,
            2.6,
            1.5,
            1.8,
            0.5,
            0.4,
            0,
            1,
            1.4,
            0.4,
            1.6,
            0.6,
            0.8,
            1.2,
            0,
            0.4,
            0,
            0.5,
            1.4,
            1.4,
            0,
            1.6,
            0.8,
            0.8,
            1.5,
            0.2,
            3,
            0.4,
            0,
            0.2,
            0,
            0,
            0,
            0,
            0.5,
            0.4,
            1.8,
            0.6,
            0,
            0.8,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1.4,
            1.2,
            0.6,
            0,
            0,
            0.4,
            0,
            0,
            0,
            0.2,
            1.4,
            2.4,
            0,
            0,
            0.6,
            0,
            0,
            0,
            1.2,
            0.6,
            1.6,
            1,
            0,
            1.6,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1.2,
            0.1,
            1.9,
            0,
            0.8,
            4.2,
            0,
            0.8,
            0,
            1.5,
            0.1,
            0.2,
            1.1,
            0,
            0,
            0.2,
            0.2,
            0,
            0,
            0,
            2,
            1.9,
            0,
            0,
            2,
            0,
            0,
            0,
            0,
            0.7,
            0.1,
            0,
            0.1,
            0.2,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1.5,
            0.2,
            0.6,
            1.2,
            0,
            0.3,
            1.1,
            0,
            0.3,
            0.9,
            0,
            0,
            2.3,
            1.6,
            0.6,
            0,
            0,
            0.6,
            0,
            0,
            0.4,
            0,
            0,
            1.2,
            0,
            0,
            0,
            1.5,
            2.6,
            3.6,
            1.4,
            3.1,
            0.6,
            1,
            1.8,
            3.2,
            2.4,
            2,
            1.4,
            0,
            2.5,
            0.6,
            1.2,
            1,
            0,
            2.5,
            2.6,
            0,
            1.4,
            2.2,
            0.6,
            0,
            1.2,
            2.2,
            1.4,
            2.8,
            3,
            3.4,
            3.6,
            0.2,
            1.8,
            0.6,
            0,
            2.8,
            0.8,
            1.6,
            6.2,
            0,
            1.2,
            2.6,
            2,
            0,
            0.4,
            3.6,
            1.2,
            1,
            1.2,
            3,
            1.2,
            1.8,
            2.8,
            0,
            4,
            5.6,
            1.4,
            4,
            2.8,
            2.6,
            1.4,
            1.6,
            0.2,
            1.8,
            0,
            1,
            0.8,
            2.2,
            2.4,
            1.6,
            0,
            1.2,
            0,
            0,
            2.9,
            0,
            2,
            1.2,
            2.1,
            0.5,
            1.9,
            0,
            0,
            2,
            4.2,
            0.1,
            1.9,
            0.9,
            0,
            0,
            3,
            0.9,
            1.4,
            3.8,
            1,
            0,
            2,
            1.8,
            0,
            0.1,
            3.4,
            0.8,
            3.2,
            1.6,
            0.8,
            2.6,
            1,
            0.1,
            1,
            1,
            2,
            0.3,
            0,
            3.6,
            1.8,
            1,
            2.2,
            0,
            1.9,
            1.8,
            0.8,
            0,
            3,
            2,
            0,
            4.4,
            2.8,
            0.8,
            2.8,
            4,
            0,
            1,
            0.2,
            1.2,
            3.4,
            1.2,
            0
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "slope",
           "values": [
            0,
            0,
            2,
            2,
            2,
            1,
            1,
            2,
            2,
            2,
            2,
            2,
            2,
            1,
            2,
            1,
            2,
            0,
            2,
            2,
            1,
            2,
            2,
            1,
            2,
            2,
            2,
            2,
            2,
            0,
            2,
            2,
            2,
            0,
            2,
            0,
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            1,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            1,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            2,
            2,
            1,
            1,
            2,
            2,
            1,
            2,
            2,
            2,
            1,
            1,
            1,
            2,
            2,
            1,
            2,
            2,
            2,
            1,
            1,
            1,
            2,
            2,
            1,
            1,
            2,
            2,
            2,
            2,
            1,
            2,
            1,
            2,
            2,
            2,
            2,
            0,
            2,
            0,
            2,
            1,
            1,
            1,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            2,
            1,
            1,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            1,
            2,
            2,
            1,
            2,
            2,
            2,
            2,
            2,
            2,
            1,
            1,
            2,
            1,
            1,
            2,
            1,
            2,
            1,
            2,
            2,
            2,
            2,
            1,
            1,
            1,
            1,
            1,
            2,
            2,
            1,
            2,
            0,
            2,
            2,
            2,
            2,
            1,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            2,
            1,
            1,
            2,
            2,
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            2,
            2,
            1,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            2,
            2,
            1,
            2,
            1,
            0,
            2,
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            1,
            2,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            0,
            1,
            2,
            2,
            1,
            2,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            2,
            1,
            1,
            1,
            2,
            1,
            2,
            1,
            0,
            2,
            1,
            1,
            2,
            1,
            2,
            1,
            2,
            2,
            1,
            1,
            1,
            1,
            2,
            2,
            1,
            2,
            2,
            1,
            1,
            2,
            0,
            1,
            1,
            0,
            2,
            1,
            1,
            1,
            1,
            1,
            1,
            1
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "ca",
           "values": [
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            2,
            0,
            0,
            0,
            0,
            0,
            2,
            0,
            0,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            3,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            2,
            0,
            4,
            1,
            0,
            0,
            0,
            3,
            1,
            3,
            2,
            0,
            2,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            2,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            2,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            4,
            0,
            0,
            0,
            0,
            4,
            4,
            3,
            2,
            2,
            1,
            0,
            1,
            0,
            0,
            2,
            2,
            0,
            2,
            0,
            0,
            1,
            1,
            3,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            0,
            0,
            3,
            1,
            2,
            0,
            0,
            0,
            2,
            2,
            2,
            1,
            1,
            0,
            0,
            3,
            1,
            1,
            2,
            3,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            1,
            3,
            1,
            2,
            3,
            0,
            1,
            2,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            3,
            1,
            1,
            3,
            0,
            2,
            2,
            3,
            0,
            1,
            0,
            2,
            1,
            1,
            0,
            2,
            3,
            1,
            3,
            3,
            4,
            3,
            2,
            0,
            3,
            2,
            0,
            0,
            0,
            2,
            1,
            2,
            2,
            1,
            1,
            0,
            3,
            2,
            0,
            0,
            2,
            0,
            1,
            1,
            2,
            1,
            0,
            2,
            1,
            0,
            0,
            1,
            0,
            1,
            2,
            2,
            1,
            1,
            1,
            1,
            3,
            2,
            0,
            0,
            2,
            0,
            2,
            0,
            0,
            2,
            1,
            1
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "thal",
           "values": [
            1,
            2,
            2,
            2,
            2,
            1,
            2,
            3,
            3,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            3,
            2,
            2,
            2,
            3,
            2,
            2,
            2,
            2,
            2,
            2,
            3,
            2,
            2,
            2,
            2,
            2,
            3,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            0,
            2,
            2,
            2,
            3,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            3,
            1,
            1,
            2,
            2,
            2,
            2,
            2,
            2,
            3,
            3,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            3,
            2,
            2,
            2,
            3,
            2,
            3,
            3,
            3,
            2,
            2,
            2,
            3,
            2,
            2,
            2,
            3,
            2,
            3,
            2,
            2,
            2,
            3,
            2,
            3,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            3,
            3,
            3,
            2,
            2,
            2,
            3,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            1,
            3,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            2,
            1,
            2,
            3,
            2,
            2,
            2,
            2,
            2,
            3,
            3,
            2,
            2,
            2,
            2,
            2,
            2,
            3,
            2,
            3,
            3,
            1,
            3,
            2,
            3,
            3,
            3,
            3,
            2,
            3,
            1,
            3,
            3,
            2,
            3,
            3,
            2,
            3,
            3,
            3,
            3,
            3,
            3,
            3,
            3,
            2,
            3,
            2,
            3,
            3,
            1,
            2,
            3,
            3,
            3,
            3,
            3,
            3,
            3,
            3,
            3,
            3,
            3,
            3,
            3,
            2,
            3,
            3,
            3,
            3,
            3,
            3,
            3,
            2,
            3,
            3,
            3,
            3,
            3,
            3,
            3,
            2,
            3,
            3,
            2,
            2,
            3,
            3,
            3,
            2,
            3,
            3,
            2,
            1,
            3,
            1,
            3,
            3,
            1,
            3,
            3,
            3,
            3,
            2,
            2,
            2,
            3,
            3,
            3,
            2,
            3,
            3,
            2,
            3,
            2,
            2,
            2,
            2,
            2,
            2,
            3,
            3,
            2,
            2,
            3,
            2,
            3,
            3,
            3,
            2,
            2,
            1,
            0,
            1,
            3,
            3,
            3,
            2,
            2,
            3,
            3,
            3,
            1,
            1,
            3,
            1,
            3,
            2,
            1,
            3,
            3,
            3,
            3,
            2
           ]
          }
         ],
         "hovertemplate": "%{xaxis.title.text}=%{x}<br>%{yaxis.title.text}=%{y}<br>target=%{marker.color}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": [
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          "coloraxis": "coloraxis",
          "symbol": "circle"
         },
         "name": "",
         "showlegend": false,
         "showupperhalf": false,
         "type": "splom"
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "target"
          }
         },
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "dragmode": "select",
        "height": 1200,
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Iris Data set"
        },
        "width": 1200
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.scatter_matrix(df, dimensions=[\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\"], color=\"target\")\r\n",
    "fig.update_traces(diagonal_visible=False, showupperhalf=False)\r\n",
    "fig.update_layout(title='Iris Data set', width=1200, height=1200)\r\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_3bd1a_row0_col0,#T_3bd1a_row1_col1,#T_3bd1a_row2_col2,#T_3bd1a_row3_col3,#T_3bd1a_row4_col4,#T_3bd1a_row5_col5,#T_3bd1a_row6_col6,#T_3bd1a_row7_col7,#T_3bd1a_row8_col8,#T_3bd1a_row9_col9,#T_3bd1a_row10_col10,#T_3bd1a_row11_col11,#T_3bd1a_row12_col12{\n",
       "            background-color:  #b40426;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_3bd1a_row0_col1,#T_3bd1a_row9_col5{\n",
       "            background-color:  #5470de;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row0_col2,#T_3bd1a_row4_col11,#T_3bd1a_row6_col8{\n",
       "            background-color:  #86a9fc;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row0_col3,#T_3bd1a_row9_col11{\n",
       "            background-color:  #b2ccfb;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row0_col4,#T_3bd1a_row12_col1{\n",
       "            background-color:  #adc9fd;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row0_col5{\n",
       "            background-color:  #779af7;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row0_col6{\n",
       "            background-color:  #4358cb;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_3bd1a_row0_col7,#T_3bd1a_row1_col4,#T_3bd1a_row2_col8,#T_3bd1a_row2_col12,#T_3bd1a_row4_col1,#T_3bd1a_row4_col6,#T_3bd1a_row6_col5,#T_3bd1a_row7_col0,#T_3bd1a_row7_col11,#T_3bd1a_row8_col2,#T_3bd1a_row9_col10,#T_3bd1a_row10_col3,#T_3bd1a_row10_col9{\n",
       "            background-color:  #3b4cc0;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_3bd1a_row0_col8,#T_3bd1a_row8_col0{\n",
       "            background-color:  #b1cbfc;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row0_col9,#T_3bd1a_row2_col7,#T_3bd1a_row12_col9{\n",
       "            background-color:  #dcdddd;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row0_col10,#T_3bd1a_row3_col11{\n",
       "            background-color:  #90b2fe;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row0_col11{\n",
       "            background-color:  #c1d4f4;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row0_col12,#T_3bd1a_row11_col3{\n",
       "            background-color:  #7a9df8;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row1_col0,#T_3bd1a_row10_col6{\n",
       "            background-color:  #80a3fa;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row1_col2,#T_3bd1a_row12_col4{\n",
       "            background-color:  #8caffe;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row1_col3,#T_3bd1a_row5_col6,#T_3bd1a_row7_col12{\n",
       "            background-color:  #4b64d5;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_3bd1a_row1_col5,#T_3bd1a_row3_col1,#T_3bd1a_row12_col6{\n",
       "            background-color:  #5f7fe8;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row1_col6,#T_3bd1a_row9_col6{\n",
       "            background-color:  #536edd;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row1_col7,#T_3bd1a_row3_col7{\n",
       "            background-color:  #8db0fe;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row1_col8{\n",
       "            background-color:  #bbd1f8;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row1_col9{\n",
       "            background-color:  #c9d7f0;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row1_col10{\n",
       "            background-color:  #aec9fc;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row1_col11,#T_3bd1a_row2_col9,#T_3bd1a_row8_col11,#T_3bd1a_row11_col12{\n",
       "            background-color:  #94b6ff;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row1_col12,#T_3bd1a_row3_col2,#T_3bd1a_row6_col7,#T_3bd1a_row8_col12,#T_3bd1a_row9_col12{\n",
       "            background-color:  #a5c3fe;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row2_col0{\n",
       "            background-color:  #88abfd;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row2_col1{\n",
       "            background-color:  #6180e9;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row2_col3{\n",
       "            background-color:  #6a8bef;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row2_col4,#T_3bd1a_row8_col5,#T_3bd1a_row10_col8{\n",
       "            background-color:  #5977e3;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row2_col5,#T_3bd1a_row10_col0,#T_3bd1a_row12_col2{\n",
       "            background-color:  #6f92f3;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row2_col6,#T_3bd1a_row7_col6,#T_3bd1a_row8_col3{\n",
       "            background-color:  #7093f3;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row2_col10{\n",
       "            background-color:  #cdd9ec;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row2_col11{\n",
       "            background-color:  #4257c9;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_3bd1a_row3_col0{\n",
       "            background-color:  #d9dce1;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row3_col4,#T_3bd1a_row5_col3{\n",
       "            background-color:  #93b5fe;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row3_col5{\n",
       "            background-color:  #89acfd;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row3_col6{\n",
       "            background-color:  #445acc;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_3bd1a_row3_col8,#T_3bd1a_row4_col8,#T_3bd1a_row5_col10,#T_3bd1a_row6_col9{\n",
       "            background-color:  #a9c6fd;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row3_col9,#T_3bd1a_row9_col8{\n",
       "            background-color:  #dadce0;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row3_col10,#T_3bd1a_row5_col11{\n",
       "            background-color:  #9bbcff;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row3_col12{\n",
       "            background-color:  #799cf8;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row4_col0{\n",
       "            background-color:  #ccd9ed;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row4_col2{\n",
       "            background-color:  #85a8fc;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row4_col3,#T_3bd1a_row12_col7{\n",
       "            background-color:  #81a4fb;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row4_col5{\n",
       "            background-color:  #5673e0;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row4_col7,#T_3bd1a_row5_col7,#T_3bd1a_row9_col3{\n",
       "            background-color:  #97b8ff;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row4_col9{\n",
       "            background-color:  #c0d4f5;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row4_col10,#T_3bd1a_row11_col8{\n",
       "            background-color:  #b5cdfa;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row4_col12,#T_3bd1a_row11_col4{\n",
       "            background-color:  #84a7fc;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row5_col0{\n",
       "            background-color:  #b7cff9;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row5_col1,#T_3bd1a_row6_col0,#T_3bd1a_row8_col10{\n",
       "            background-color:  #7b9ff9;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row5_col2{\n",
       "            background-color:  #afcafc;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row5_col4{\n",
       "            background-color:  #7396f5;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row5_col8,#T_3bd1a_row12_col11{\n",
       "            background-color:  #9fbfff;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row5_col9,#T_3bd1a_row10_col2{\n",
       "            background-color:  #b6cefa;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row5_col12,#T_3bd1a_row10_col11{\n",
       "            background-color:  #5d7ce6;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row6_col1,#T_3bd1a_row6_col11{\n",
       "            background-color:  #5e7de7;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row6_col2,#T_3bd1a_row11_col10{\n",
       "            background-color:  #a3c2fe;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row6_col3{\n",
       "            background-color:  #3c4ec2;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_3bd1a_row6_col4{\n",
       "            background-color:  #465ecf;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_3bd1a_row6_col10{\n",
       "            background-color:  #c7d7f0;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row6_col12,#T_3bd1a_row7_col1{\n",
       "            background-color:  #6282ea;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row7_col2{\n",
       "            background-color:  #dbdcde;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row7_col3,#T_3bd1a_row7_col5,#T_3bd1a_row8_col6,#T_3bd1a_row11_col6{\n",
       "            background-color:  #4f69d9;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row7_col4{\n",
       "            background-color:  #6c8ff1;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row7_col8{\n",
       "            background-color:  #3d50c3;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_3bd1a_row7_col9{\n",
       "            background-color:  #688aef;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row7_col10{\n",
       "            background-color:  #f3c8b2;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row8_col1{\n",
       "            background-color:  #98b9ff;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row8_col4{\n",
       "            background-color:  #82a6fb;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row8_col7{\n",
       "            background-color:  #3e51c5;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_3bd1a_row8_col9{\n",
       "            background-color:  #e9d5cb;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row9_col0{\n",
       "            background-color:  #cbd8ee;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row9_col1{\n",
       "            background-color:  #8badfd;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row9_col2{\n",
       "            background-color:  #7295f4;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row9_col4{\n",
       "            background-color:  #7ea1fa;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row9_col7{\n",
       "            background-color:  #455cce;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_3bd1a_row10_col1{\n",
       "            background-color:  #6687ed;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row10_col4,#T_3bd1a_row12_col3{\n",
       "            background-color:  #6e90f2;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row10_col5{\n",
       "            background-color:  #4055c8;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_3bd1a_row10_col7{\n",
       "            background-color:  #ebd3c6;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row10_col12,#T_3bd1a_row12_col5{\n",
       "            background-color:  #4961d2;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_3bd1a_row11_col0{\n",
       "            background-color:  #d8dce2;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row11_col1{\n",
       "            background-color:  #92b4fe;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row11_col2{\n",
       "            background-color:  #6b8df0;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row11_col5{\n",
       "            background-color:  #7da0f9;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row11_col7{\n",
       "            background-color:  #6384eb;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row11_col9{\n",
       "            background-color:  #dedcdb;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row12_col0{\n",
       "            background-color:  #aac7fd;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row12_col8{\n",
       "            background-color:  #cad8ef;\n",
       "            color:  #000000;\n",
       "        }#T_3bd1a_row12_col10{\n",
       "            background-color:  #9ebeff;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_3bd1a_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >age</th>        <th class=\"col_heading level0 col1\" >sex</th>        <th class=\"col_heading level0 col2\" >cp</th>        <th class=\"col_heading level0 col3\" >trestbps</th>        <th class=\"col_heading level0 col4\" >chol</th>        <th class=\"col_heading level0 col5\" >fbs</th>        <th class=\"col_heading level0 col6\" >restecg</th>        <th class=\"col_heading level0 col7\" >thalach</th>        <th class=\"col_heading level0 col8\" >exang</th>        <th class=\"col_heading level0 col9\" >oldpeak</th>        <th class=\"col_heading level0 col10\" >slope</th>        <th class=\"col_heading level0 col11\" >ca</th>        <th class=\"col_heading level0 col12\" >thal</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_3bd1a_level0_row0\" class=\"row_heading level0 row0\" >age</th>\n",
       "                        <td id=\"T_3bd1a_row0_col0\" class=\"data row0 col0\" >1.00</td>\n",
       "                        <td id=\"T_3bd1a_row0_col1\" class=\"data row0 col1\" >-0.10</td>\n",
       "                        <td id=\"T_3bd1a_row0_col2\" class=\"data row0 col2\" >-0.07</td>\n",
       "                        <td id=\"T_3bd1a_row0_col3\" class=\"data row0 col3\" >0.28</td>\n",
       "                        <td id=\"T_3bd1a_row0_col4\" class=\"data row0 col4\" >0.21</td>\n",
       "                        <td id=\"T_3bd1a_row0_col5\" class=\"data row0 col5\" >0.12</td>\n",
       "                        <td id=\"T_3bd1a_row0_col6\" class=\"data row0 col6\" >-0.12</td>\n",
       "                        <td id=\"T_3bd1a_row0_col7\" class=\"data row0 col7\" >-0.40</td>\n",
       "                        <td id=\"T_3bd1a_row0_col8\" class=\"data row0 col8\" >0.10</td>\n",
       "                        <td id=\"T_3bd1a_row0_col9\" class=\"data row0 col9\" >0.21</td>\n",
       "                        <td id=\"T_3bd1a_row0_col10\" class=\"data row0 col10\" >-0.17</td>\n",
       "                        <td id=\"T_3bd1a_row0_col11\" class=\"data row0 col11\" >0.28</td>\n",
       "                        <td id=\"T_3bd1a_row0_col12\" class=\"data row0 col12\" >0.07</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3bd1a_level0_row1\" class=\"row_heading level0 row1\" >sex</th>\n",
       "                        <td id=\"T_3bd1a_row1_col0\" class=\"data row1 col0\" >-0.10</td>\n",
       "                        <td id=\"T_3bd1a_row1_col1\" class=\"data row1 col1\" >1.00</td>\n",
       "                        <td id=\"T_3bd1a_row1_col2\" class=\"data row1 col2\" >-0.05</td>\n",
       "                        <td id=\"T_3bd1a_row1_col3\" class=\"data row1 col3\" >-0.06</td>\n",
       "                        <td id=\"T_3bd1a_row1_col4\" class=\"data row1 col4\" >-0.20</td>\n",
       "                        <td id=\"T_3bd1a_row1_col5\" class=\"data row1 col5\" >0.05</td>\n",
       "                        <td id=\"T_3bd1a_row1_col6\" class=\"data row1 col6\" >-0.06</td>\n",
       "                        <td id=\"T_3bd1a_row1_col7\" class=\"data row1 col7\" >-0.04</td>\n",
       "                        <td id=\"T_3bd1a_row1_col8\" class=\"data row1 col8\" >0.14</td>\n",
       "                        <td id=\"T_3bd1a_row1_col9\" class=\"data row1 col9\" >0.10</td>\n",
       "                        <td id=\"T_3bd1a_row1_col10\" class=\"data row1 col10\" >-0.03</td>\n",
       "                        <td id=\"T_3bd1a_row1_col11\" class=\"data row1 col11\" >0.12</td>\n",
       "                        <td id=\"T_3bd1a_row1_col12\" class=\"data row1 col12\" >0.21</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3bd1a_level0_row2\" class=\"row_heading level0 row2\" >cp</th>\n",
       "                        <td id=\"T_3bd1a_row2_col0\" class=\"data row2 col0\" >-0.07</td>\n",
       "                        <td id=\"T_3bd1a_row2_col1\" class=\"data row2 col1\" >-0.05</td>\n",
       "                        <td id=\"T_3bd1a_row2_col2\" class=\"data row2 col2\" >1.00</td>\n",
       "                        <td id=\"T_3bd1a_row2_col3\" class=\"data row2 col3\" >0.05</td>\n",
       "                        <td id=\"T_3bd1a_row2_col4\" class=\"data row2 col4\" >-0.08</td>\n",
       "                        <td id=\"T_3bd1a_row2_col5\" class=\"data row2 col5\" >0.09</td>\n",
       "                        <td id=\"T_3bd1a_row2_col6\" class=\"data row2 col6\" >0.04</td>\n",
       "                        <td id=\"T_3bd1a_row2_col7\" class=\"data row2 col7\" >0.30</td>\n",
       "                        <td id=\"T_3bd1a_row2_col8\" class=\"data row2 col8\" >-0.39</td>\n",
       "                        <td id=\"T_3bd1a_row2_col9\" class=\"data row2 col9\" >-0.15</td>\n",
       "                        <td id=\"T_3bd1a_row2_col10\" class=\"data row2 col10\" >0.12</td>\n",
       "                        <td id=\"T_3bd1a_row2_col11\" class=\"data row2 col11\" >-0.18</td>\n",
       "                        <td id=\"T_3bd1a_row2_col12\" class=\"data row2 col12\" >-0.16</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3bd1a_level0_row3\" class=\"row_heading level0 row3\" >trestbps</th>\n",
       "                        <td id=\"T_3bd1a_row3_col0\" class=\"data row3 col0\" >0.28</td>\n",
       "                        <td id=\"T_3bd1a_row3_col1\" class=\"data row3 col1\" >-0.06</td>\n",
       "                        <td id=\"T_3bd1a_row3_col2\" class=\"data row3 col2\" >0.05</td>\n",
       "                        <td id=\"T_3bd1a_row3_col3\" class=\"data row3 col3\" >1.00</td>\n",
       "                        <td id=\"T_3bd1a_row3_col4\" class=\"data row3 col4\" >0.12</td>\n",
       "                        <td id=\"T_3bd1a_row3_col5\" class=\"data row3 col5\" >0.18</td>\n",
       "                        <td id=\"T_3bd1a_row3_col6\" class=\"data row3 col6\" >-0.11</td>\n",
       "                        <td id=\"T_3bd1a_row3_col7\" class=\"data row3 col7\" >-0.05</td>\n",
       "                        <td id=\"T_3bd1a_row3_col8\" class=\"data row3 col8\" >0.07</td>\n",
       "                        <td id=\"T_3bd1a_row3_col9\" class=\"data row3 col9\" >0.19</td>\n",
       "                        <td id=\"T_3bd1a_row3_col10\" class=\"data row3 col10\" >-0.12</td>\n",
       "                        <td id=\"T_3bd1a_row3_col11\" class=\"data row3 col11\" >0.10</td>\n",
       "                        <td id=\"T_3bd1a_row3_col12\" class=\"data row3 col12\" >0.06</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3bd1a_level0_row4\" class=\"row_heading level0 row4\" >chol</th>\n",
       "                        <td id=\"T_3bd1a_row4_col0\" class=\"data row4 col0\" >0.21</td>\n",
       "                        <td id=\"T_3bd1a_row4_col1\" class=\"data row4 col1\" >-0.20</td>\n",
       "                        <td id=\"T_3bd1a_row4_col2\" class=\"data row4 col2\" >-0.08</td>\n",
       "                        <td id=\"T_3bd1a_row4_col3\" class=\"data row4 col3\" >0.12</td>\n",
       "                        <td id=\"T_3bd1a_row4_col4\" class=\"data row4 col4\" >1.00</td>\n",
       "                        <td id=\"T_3bd1a_row4_col5\" class=\"data row4 col5\" >0.01</td>\n",
       "                        <td id=\"T_3bd1a_row4_col6\" class=\"data row4 col6\" >-0.15</td>\n",
       "                        <td id=\"T_3bd1a_row4_col7\" class=\"data row4 col7\" >-0.01</td>\n",
       "                        <td id=\"T_3bd1a_row4_col8\" class=\"data row4 col8\" >0.07</td>\n",
       "                        <td id=\"T_3bd1a_row4_col9\" class=\"data row4 col9\" >0.05</td>\n",
       "                        <td id=\"T_3bd1a_row4_col10\" class=\"data row4 col10\" >-0.00</td>\n",
       "                        <td id=\"T_3bd1a_row4_col11\" class=\"data row4 col11\" >0.07</td>\n",
       "                        <td id=\"T_3bd1a_row4_col12\" class=\"data row4 col12\" >0.10</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3bd1a_level0_row5\" class=\"row_heading level0 row5\" >fbs</th>\n",
       "                        <td id=\"T_3bd1a_row5_col0\" class=\"data row5 col0\" >0.12</td>\n",
       "                        <td id=\"T_3bd1a_row5_col1\" class=\"data row5 col1\" >0.05</td>\n",
       "                        <td id=\"T_3bd1a_row5_col2\" class=\"data row5 col2\" >0.09</td>\n",
       "                        <td id=\"T_3bd1a_row5_col3\" class=\"data row5 col3\" >0.18</td>\n",
       "                        <td id=\"T_3bd1a_row5_col4\" class=\"data row5 col4\" >0.01</td>\n",
       "                        <td id=\"T_3bd1a_row5_col5\" class=\"data row5 col5\" >1.00</td>\n",
       "                        <td id=\"T_3bd1a_row5_col6\" class=\"data row5 col6\" >-0.08</td>\n",
       "                        <td id=\"T_3bd1a_row5_col7\" class=\"data row5 col7\" >-0.01</td>\n",
       "                        <td id=\"T_3bd1a_row5_col8\" class=\"data row5 col8\" >0.03</td>\n",
       "                        <td id=\"T_3bd1a_row5_col9\" class=\"data row5 col9\" >0.01</td>\n",
       "                        <td id=\"T_3bd1a_row5_col10\" class=\"data row5 col10\" >-0.06</td>\n",
       "                        <td id=\"T_3bd1a_row5_col11\" class=\"data row5 col11\" >0.14</td>\n",
       "                        <td id=\"T_3bd1a_row5_col12\" class=\"data row5 col12\" >-0.03</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3bd1a_level0_row6\" class=\"row_heading level0 row6\" >restecg</th>\n",
       "                        <td id=\"T_3bd1a_row6_col0\" class=\"data row6 col0\" >-0.12</td>\n",
       "                        <td id=\"T_3bd1a_row6_col1\" class=\"data row6 col1\" >-0.06</td>\n",
       "                        <td id=\"T_3bd1a_row6_col2\" class=\"data row6 col2\" >0.04</td>\n",
       "                        <td id=\"T_3bd1a_row6_col3\" class=\"data row6 col3\" >-0.11</td>\n",
       "                        <td id=\"T_3bd1a_row6_col4\" class=\"data row6 col4\" >-0.15</td>\n",
       "                        <td id=\"T_3bd1a_row6_col5\" class=\"data row6 col5\" >-0.08</td>\n",
       "                        <td id=\"T_3bd1a_row6_col6\" class=\"data row6 col6\" >1.00</td>\n",
       "                        <td id=\"T_3bd1a_row6_col7\" class=\"data row6 col7\" >0.04</td>\n",
       "                        <td id=\"T_3bd1a_row6_col8\" class=\"data row6 col8\" >-0.07</td>\n",
       "                        <td id=\"T_3bd1a_row6_col9\" class=\"data row6 col9\" >-0.06</td>\n",
       "                        <td id=\"T_3bd1a_row6_col10\" class=\"data row6 col10\" >0.09</td>\n",
       "                        <td id=\"T_3bd1a_row6_col11\" class=\"data row6 col11\" >-0.07</td>\n",
       "                        <td id=\"T_3bd1a_row6_col12\" class=\"data row6 col12\" >-0.01</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3bd1a_level0_row7\" class=\"row_heading level0 row7\" >thalach</th>\n",
       "                        <td id=\"T_3bd1a_row7_col0\" class=\"data row7 col0\" >-0.40</td>\n",
       "                        <td id=\"T_3bd1a_row7_col1\" class=\"data row7 col1\" >-0.04</td>\n",
       "                        <td id=\"T_3bd1a_row7_col2\" class=\"data row7 col2\" >0.30</td>\n",
       "                        <td id=\"T_3bd1a_row7_col3\" class=\"data row7 col3\" >-0.05</td>\n",
       "                        <td id=\"T_3bd1a_row7_col4\" class=\"data row7 col4\" >-0.01</td>\n",
       "                        <td id=\"T_3bd1a_row7_col5\" class=\"data row7 col5\" >-0.01</td>\n",
       "                        <td id=\"T_3bd1a_row7_col6\" class=\"data row7 col6\" >0.04</td>\n",
       "                        <td id=\"T_3bd1a_row7_col7\" class=\"data row7 col7\" >1.00</td>\n",
       "                        <td id=\"T_3bd1a_row7_col8\" class=\"data row7 col8\" >-0.38</td>\n",
       "                        <td id=\"T_3bd1a_row7_col9\" class=\"data row7 col9\" >-0.34</td>\n",
       "                        <td id=\"T_3bd1a_row7_col10\" class=\"data row7 col10\" >0.39</td>\n",
       "                        <td id=\"T_3bd1a_row7_col11\" class=\"data row7 col11\" >-0.21</td>\n",
       "                        <td id=\"T_3bd1a_row7_col12\" class=\"data row7 col12\" >-0.10</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3bd1a_level0_row8\" class=\"row_heading level0 row8\" >exang</th>\n",
       "                        <td id=\"T_3bd1a_row8_col0\" class=\"data row8 col0\" >0.10</td>\n",
       "                        <td id=\"T_3bd1a_row8_col1\" class=\"data row8 col1\" >0.14</td>\n",
       "                        <td id=\"T_3bd1a_row8_col2\" class=\"data row8 col2\" >-0.39</td>\n",
       "                        <td id=\"T_3bd1a_row8_col3\" class=\"data row8 col3\" >0.07</td>\n",
       "                        <td id=\"T_3bd1a_row8_col4\" class=\"data row8 col4\" >0.07</td>\n",
       "                        <td id=\"T_3bd1a_row8_col5\" class=\"data row8 col5\" >0.03</td>\n",
       "                        <td id=\"T_3bd1a_row8_col6\" class=\"data row8 col6\" >-0.07</td>\n",
       "                        <td id=\"T_3bd1a_row8_col7\" class=\"data row8 col7\" >-0.38</td>\n",
       "                        <td id=\"T_3bd1a_row8_col8\" class=\"data row8 col8\" >1.00</td>\n",
       "                        <td id=\"T_3bd1a_row8_col9\" class=\"data row8 col9\" >0.29</td>\n",
       "                        <td id=\"T_3bd1a_row8_col10\" class=\"data row8 col10\" >-0.26</td>\n",
       "                        <td id=\"T_3bd1a_row8_col11\" class=\"data row8 col11\" >0.12</td>\n",
       "                        <td id=\"T_3bd1a_row8_col12\" class=\"data row8 col12\" >0.21</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3bd1a_level0_row9\" class=\"row_heading level0 row9\" >oldpeak</th>\n",
       "                        <td id=\"T_3bd1a_row9_col0\" class=\"data row9 col0\" >0.21</td>\n",
       "                        <td id=\"T_3bd1a_row9_col1\" class=\"data row9 col1\" >0.10</td>\n",
       "                        <td id=\"T_3bd1a_row9_col2\" class=\"data row9 col2\" >-0.15</td>\n",
       "                        <td id=\"T_3bd1a_row9_col3\" class=\"data row9 col3\" >0.19</td>\n",
       "                        <td id=\"T_3bd1a_row9_col4\" class=\"data row9 col4\" >0.05</td>\n",
       "                        <td id=\"T_3bd1a_row9_col5\" class=\"data row9 col5\" >0.01</td>\n",
       "                        <td id=\"T_3bd1a_row9_col6\" class=\"data row9 col6\" >-0.06</td>\n",
       "                        <td id=\"T_3bd1a_row9_col7\" class=\"data row9 col7\" >-0.34</td>\n",
       "                        <td id=\"T_3bd1a_row9_col8\" class=\"data row9 col8\" >0.29</td>\n",
       "                        <td id=\"T_3bd1a_row9_col9\" class=\"data row9 col9\" >1.00</td>\n",
       "                        <td id=\"T_3bd1a_row9_col10\" class=\"data row9 col10\" >-0.58</td>\n",
       "                        <td id=\"T_3bd1a_row9_col11\" class=\"data row9 col11\" >0.22</td>\n",
       "                        <td id=\"T_3bd1a_row9_col12\" class=\"data row9 col12\" >0.21</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3bd1a_level0_row10\" class=\"row_heading level0 row10\" >slope</th>\n",
       "                        <td id=\"T_3bd1a_row10_col0\" class=\"data row10 col0\" >-0.17</td>\n",
       "                        <td id=\"T_3bd1a_row10_col1\" class=\"data row10 col1\" >-0.03</td>\n",
       "                        <td id=\"T_3bd1a_row10_col2\" class=\"data row10 col2\" >0.12</td>\n",
       "                        <td id=\"T_3bd1a_row10_col3\" class=\"data row10 col3\" >-0.12</td>\n",
       "                        <td id=\"T_3bd1a_row10_col4\" class=\"data row10 col4\" >-0.00</td>\n",
       "                        <td id=\"T_3bd1a_row10_col5\" class=\"data row10 col5\" >-0.06</td>\n",
       "                        <td id=\"T_3bd1a_row10_col6\" class=\"data row10 col6\" >0.09</td>\n",
       "                        <td id=\"T_3bd1a_row10_col7\" class=\"data row10 col7\" >0.39</td>\n",
       "                        <td id=\"T_3bd1a_row10_col8\" class=\"data row10 col8\" >-0.26</td>\n",
       "                        <td id=\"T_3bd1a_row10_col9\" class=\"data row10 col9\" >-0.58</td>\n",
       "                        <td id=\"T_3bd1a_row10_col10\" class=\"data row10 col10\" >1.00</td>\n",
       "                        <td id=\"T_3bd1a_row10_col11\" class=\"data row10 col11\" >-0.08</td>\n",
       "                        <td id=\"T_3bd1a_row10_col12\" class=\"data row10 col12\" >-0.10</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3bd1a_level0_row11\" class=\"row_heading level0 row11\" >ca</th>\n",
       "                        <td id=\"T_3bd1a_row11_col0\" class=\"data row11 col0\" >0.28</td>\n",
       "                        <td id=\"T_3bd1a_row11_col1\" class=\"data row11 col1\" >0.12</td>\n",
       "                        <td id=\"T_3bd1a_row11_col2\" class=\"data row11 col2\" >-0.18</td>\n",
       "                        <td id=\"T_3bd1a_row11_col3\" class=\"data row11 col3\" >0.10</td>\n",
       "                        <td id=\"T_3bd1a_row11_col4\" class=\"data row11 col4\" >0.07</td>\n",
       "                        <td id=\"T_3bd1a_row11_col5\" class=\"data row11 col5\" >0.14</td>\n",
       "                        <td id=\"T_3bd1a_row11_col6\" class=\"data row11 col6\" >-0.07</td>\n",
       "                        <td id=\"T_3bd1a_row11_col7\" class=\"data row11 col7\" >-0.21</td>\n",
       "                        <td id=\"T_3bd1a_row11_col8\" class=\"data row11 col8\" >0.12</td>\n",
       "                        <td id=\"T_3bd1a_row11_col9\" class=\"data row11 col9\" >0.22</td>\n",
       "                        <td id=\"T_3bd1a_row11_col10\" class=\"data row11 col10\" >-0.08</td>\n",
       "                        <td id=\"T_3bd1a_row11_col11\" class=\"data row11 col11\" >1.00</td>\n",
       "                        <td id=\"T_3bd1a_row11_col12\" class=\"data row11 col12\" >0.15</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3bd1a_level0_row12\" class=\"row_heading level0 row12\" >thal</th>\n",
       "                        <td id=\"T_3bd1a_row12_col0\" class=\"data row12 col0\" >0.07</td>\n",
       "                        <td id=\"T_3bd1a_row12_col1\" class=\"data row12 col1\" >0.21</td>\n",
       "                        <td id=\"T_3bd1a_row12_col2\" class=\"data row12 col2\" >-0.16</td>\n",
       "                        <td id=\"T_3bd1a_row12_col3\" class=\"data row12 col3\" >0.06</td>\n",
       "                        <td id=\"T_3bd1a_row12_col4\" class=\"data row12 col4\" >0.10</td>\n",
       "                        <td id=\"T_3bd1a_row12_col5\" class=\"data row12 col5\" >-0.03</td>\n",
       "                        <td id=\"T_3bd1a_row12_col6\" class=\"data row12 col6\" >-0.01</td>\n",
       "                        <td id=\"T_3bd1a_row12_col7\" class=\"data row12 col7\" >-0.10</td>\n",
       "                        <td id=\"T_3bd1a_row12_col8\" class=\"data row12 col8\" >0.21</td>\n",
       "                        <td id=\"T_3bd1a_row12_col9\" class=\"data row12 col9\" >0.21</td>\n",
       "                        <td id=\"T_3bd1a_row12_col10\" class=\"data row12 col10\" >-0.10</td>\n",
       "                        <td id=\"T_3bd1a_row12_col11\" class=\"data row12 col11\" >0.15</td>\n",
       "                        <td id=\"T_3bd1a_row12_col12\" class=\"data row12 col12\" >1.00</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x12fb167f100>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = df.iloc[:,0:13].corr()\r\n",
    "corr.style.background_gradient(cmap='coolwarm').set_precision(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos retirar três variáveis que estão muito correlacionadas com as outras: \"thalach\", \"exang\" e \"slope\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['thalach', 'exang', 'slope'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padronizando os dados com a função Min-Max apenas nas colunas que precisam de ajuste \r\n",
    "### Dados finais entre zero e 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70833333, 1.        , 3.        , 0.48113208, 0.24429224,\n",
       "        1.        , 0.        , 2.3       , 0.        , 1.        ,\n",
       "        1.        ],\n",
       "       [0.16666667, 1.        , 2.        , 0.33962264, 0.28310502,\n",
       "        0.        , 1.        , 3.5       , 0.        , 2.        ,\n",
       "        1.        ],\n",
       "       [0.25      , 0.        , 1.        , 0.33962264, 0.17808219,\n",
       "        0.        , 0.        , 1.4       , 0.        , 2.        ,\n",
       "        1.        ],\n",
       "       [0.5625    , 1.        , 1.        , 0.24528302, 0.25114155,\n",
       "        0.        , 1.        , 0.8       , 0.        , 2.        ,\n",
       "        1.        ],\n",
       "       [0.58333333, 0.        , 0.        , 0.24528302, 0.52054795,\n",
       "        0.        , 1.        , 0.6       , 0.        , 2.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:,0:11].copy()\r\n",
    "X.age = min_max(X.age)\r\n",
    "X.trestbps = min_max(X.trestbps)\r\n",
    "X.chol = min_max(X.chol)\r\n",
    "#X.thalach = min_max(X.thalach)\r\n",
    "X = X.to_numpy()\r\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 11)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = X.shape[1]\r\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303,)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.target.to_numpy()\r\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividindo a Base de Dados entre Treino e Teste, com 20% para os testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242, 11)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242,)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de MLP\r\n",
    "\r\n",
    "## Criando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_114 (Dense)            (None, 13)                156       \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 13)                0         \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 7)                 98        \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 7)                 0         \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 262\n",
      "Trainable params: 262\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\r\n",
    "model.add(Dense(13, input_dim=num_features, activation='relu'))\r\n",
    "model.add(Dropout(0.2))\r\n",
    "model.add(Dense(7, activation='relu'))\r\n",
    "model.add(Dropout(0.2))\r\n",
    "model.add(Dense(1 , activation='sigmoid'))\r\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilando e treinando o modelo. \r\n",
    "### Vamos utilizar a função de Callback ModelCheckPointer para salvar o modelo com a melhor accuracia na base de validação (que é a mesma de teste final pois temos uma base muito pequena)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.7824 - accuracy: 0.3750\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.31148, saving model to .\\modelo_mlp_ex3_2.hdf5\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.8435 - accuracy: 0.3884 - val_loss: 0.7853 - val_accuracy: 0.3115\n",
      "Epoch 2/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.8326 - accuracy: 0.3438\n",
      "Epoch 00002: val_accuracy did not improve from 0.31148\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8402 - accuracy: 0.3347 - val_loss: 0.7708 - val_accuracy: 0.3115\n",
      "Epoch 3/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.8219 - accuracy: 0.3281\n",
      "Epoch 00003: val_accuracy improved from 0.31148 to 0.34426, saving model to .\\modelo_mlp_ex3_2.hdf5\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.7757 - accuracy: 0.4215 - val_loss: 0.7563 - val_accuracy: 0.3443\n",
      "Epoch 4/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.7251 - accuracy: 0.4844\n",
      "Epoch 00004: val_accuracy improved from 0.34426 to 0.40984, saving model to .\\modelo_mlp_ex3_2.hdf5\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.7784 - accuracy: 0.4421 - val_loss: 0.7420 - val_accuracy: 0.4098\n",
      "Epoch 5/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.7778 - accuracy: 0.3906\n",
      "Epoch 00005: val_accuracy did not improve from 0.40984\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7683 - accuracy: 0.4463 - val_loss: 0.7279 - val_accuracy: 0.4098\n",
      "Epoch 6/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.7620 - accuracy: 0.4844\n",
      "Epoch 00006: val_accuracy did not improve from 0.40984\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7513 - accuracy: 0.4669 - val_loss: 0.7148 - val_accuracy: 0.4098\n",
      "Epoch 7/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.7642 - accuracy: 0.4375\n",
      "Epoch 00007: val_accuracy improved from 0.40984 to 0.52459, saving model to .\\modelo_mlp_ex3_2.hdf5\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.7543 - accuracy: 0.4545 - val_loss: 0.7025 - val_accuracy: 0.5246\n",
      "Epoch 8/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.7434 - accuracy: 0.4844\n",
      "Epoch 00008: val_accuracy improved from 0.52459 to 0.54098, saving model to .\\modelo_mlp_ex3_2.hdf5\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7271 - accuracy: 0.5372 - val_loss: 0.6912 - val_accuracy: 0.5410\n",
      "Epoch 9/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.7085 - accuracy: 0.5312\n",
      "Epoch 00009: val_accuracy improved from 0.54098 to 0.55738, saving model to .\\modelo_mlp_ex3_2.hdf5\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.7033 - accuracy: 0.5289 - val_loss: 0.6807 - val_accuracy: 0.5574\n",
      "Epoch 10/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.7385 - accuracy: 0.4531\n",
      "Epoch 00010: val_accuracy improved from 0.55738 to 0.57377, saving model to .\\modelo_mlp_ex3_2.hdf5\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.7154 - accuracy: 0.5083 - val_loss: 0.6702 - val_accuracy: 0.5738\n",
      "Epoch 11/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.7029 - accuracy: 0.6094\n",
      "Epoch 00011: val_accuracy improved from 0.57377 to 0.60656, saving model to .\\modelo_mlp_ex3_2.hdf5\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.7169 - accuracy: 0.5413 - val_loss: 0.6600 - val_accuracy: 0.6066\n",
      "Epoch 12/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.7257 - accuracy: 0.4531\n",
      "Epoch 00012: val_accuracy improved from 0.60656 to 0.65574, saving model to .\\modelo_mlp_ex3_2.hdf5\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.7065 - accuracy: 0.5331 - val_loss: 0.6504 - val_accuracy: 0.6557\n",
      "Epoch 13/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.6820 - accuracy: 0.6406\n",
      "Epoch 00013: val_accuracy improved from 0.65574 to 0.72131, saving model to .\\modelo_mlp_ex3_2.hdf5\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.6773 - accuracy: 0.6157 - val_loss: 0.6406 - val_accuracy: 0.7213\n",
      "Epoch 14/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.6296 - accuracy: 0.6406\n",
      "Epoch 00014: val_accuracy improved from 0.72131 to 0.77049, saving model to .\\modelo_mlp_ex3_2.hdf5\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.6466 - accuracy: 0.6322 - val_loss: 0.6310 - val_accuracy: 0.7705\n",
      "Epoch 15/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.6677 - accuracy: 0.6250\n",
      "Epoch 00015: val_accuracy improved from 0.77049 to 0.80328, saving model to .\\modelo_mlp_ex3_2.hdf5\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.6537 - accuracy: 0.5992 - val_loss: 0.6223 - val_accuracy: 0.8033\n",
      "Epoch 16/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.6373 - accuracy: 0.6719\n",
      "Epoch 00016: val_accuracy did not improve from 0.80328\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.6434 - accuracy: 0.6364 - val_loss: 0.6133 - val_accuracy: 0.7869\n",
      "Epoch 17/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.6177 - accuracy: 0.6406\n",
      "Epoch 00017: val_accuracy improved from 0.80328 to 0.81967, saving model to .\\modelo_mlp_ex3_2.hdf5\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.6373 - accuracy: 0.6364 - val_loss: 0.6043 - val_accuracy: 0.8197\n",
      "Epoch 18/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.6283 - accuracy: 0.6719\n",
      "Epoch 00018: val_accuracy did not improve from 0.81967\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.6383 - accuracy: 0.6694 - val_loss: 0.5953 - val_accuracy: 0.8197\n",
      "Epoch 19/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5882 - accuracy: 0.7031\n",
      "Epoch 00019: val_accuracy improved from 0.81967 to 0.83607, saving model to .\\modelo_mlp_ex3_2.hdf5\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.6295 - accuracy: 0.6240 - val_loss: 0.5869 - val_accuracy: 0.8361\n",
      "Epoch 20/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.6306 - accuracy: 0.6719\n",
      "Epoch 00020: val_accuracy improved from 0.83607 to 0.85246, saving model to .\\modelo_mlp_ex3_2.hdf5\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.6340 - accuracy: 0.6488 - val_loss: 0.5789 - val_accuracy: 0.8525\n",
      "Epoch 21/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.6388 - accuracy: 0.6719\n",
      "Epoch 00021: val_accuracy did not improve from 0.85246\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.6373 - accuracy: 0.6653 - val_loss: 0.5716 - val_accuracy: 0.8361\n",
      "Epoch 22/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5754 - accuracy: 0.7344\n",
      "Epoch 00022: val_accuracy did not improve from 0.85246\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.5979 - accuracy: 0.7149 - val_loss: 0.5638 - val_accuracy: 0.8361\n",
      "Epoch 23/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5931 - accuracy: 0.7344\n",
      "Epoch 00023: val_accuracy did not improve from 0.85246\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.6045 - accuracy: 0.6860 - val_loss: 0.5559 - val_accuracy: 0.8525\n",
      "Epoch 24/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.6371 - accuracy: 0.6094\n",
      "Epoch 00024: val_accuracy improved from 0.85246 to 0.88525, saving model to .\\modelo_mlp_ex3_2.hdf5\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.6140 - accuracy: 0.6818 - val_loss: 0.5489 - val_accuracy: 0.8852\n",
      "Epoch 25/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5707 - accuracy: 0.6875\n",
      "Epoch 00025: val_accuracy did not improve from 0.88525\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.5913 - accuracy: 0.6942 - val_loss: 0.5411 - val_accuracy: 0.8852\n",
      "Epoch 26/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5277 - accuracy: 0.8125\n",
      "Epoch 00026: val_accuracy did not improve from 0.88525\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.5881 - accuracy: 0.7066 - val_loss: 0.5332 - val_accuracy: 0.8852\n",
      "Epoch 27/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5906 - accuracy: 0.7188\n",
      "Epoch 00027: val_accuracy did not improve from 0.88525\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.5642 - accuracy: 0.7314 - val_loss: 0.5256 - val_accuracy: 0.8852\n",
      "Epoch 28/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5429 - accuracy: 0.7500\n",
      "Epoch 00028: val_accuracy did not improve from 0.88525\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.5689 - accuracy: 0.7314 - val_loss: 0.5180 - val_accuracy: 0.8852\n",
      "Epoch 29/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5750 - accuracy: 0.6875\n",
      "Epoch 00029: val_accuracy did not improve from 0.88525\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.5779 - accuracy: 0.6818 - val_loss: 0.5103 - val_accuracy: 0.8852\n",
      "Epoch 30/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5437 - accuracy: 0.7188\n",
      "Epoch 00030: val_accuracy improved from 0.88525 to 0.91803, saving model to .\\modelo_mlp_ex3_2.hdf5\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.5542 - accuracy: 0.7190 - val_loss: 0.5025 - val_accuracy: 0.9180\n",
      "Epoch 31/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5707 - accuracy: 0.7031\n",
      "Epoch 00031: val_accuracy did not improve from 0.91803\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.5650 - accuracy: 0.7107 - val_loss: 0.4950 - val_accuracy: 0.9180\n",
      "Epoch 32/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5169 - accuracy: 0.7500\n",
      "Epoch 00032: val_accuracy did not improve from 0.91803\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.5329 - accuracy: 0.7521 - val_loss: 0.4872 - val_accuracy: 0.9180\n",
      "Epoch 33/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5833 - accuracy: 0.7500\n",
      "Epoch 00033: val_accuracy improved from 0.91803 to 0.93443, saving model to .\\modelo_mlp_ex3_2.hdf5\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5574 - accuracy: 0.7314 - val_loss: 0.4792 - val_accuracy: 0.9344\n",
      "Epoch 34/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5484 - accuracy: 0.7344\n",
      "Epoch 00034: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.5261 - accuracy: 0.7479 - val_loss: 0.4708 - val_accuracy: 0.9344\n",
      "Epoch 35/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5945 - accuracy: 0.6875\n",
      "Epoch 00035: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.5336 - accuracy: 0.7314 - val_loss: 0.4629 - val_accuracy: 0.9344\n",
      "Epoch 36/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5468 - accuracy: 0.7656\n",
      "Epoch 00036: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.5254 - accuracy: 0.7397 - val_loss: 0.4556 - val_accuracy: 0.9344\n",
      "Epoch 37/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4903 - accuracy: 0.7656\n",
      "Epoch 00037: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.5042 - accuracy: 0.7562 - val_loss: 0.4473 - val_accuracy: 0.9344\n",
      "Epoch 38/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4675 - accuracy: 0.8281\n",
      "Epoch 00038: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.5122 - accuracy: 0.7603 - val_loss: 0.4395 - val_accuracy: 0.9344\n",
      "Epoch 39/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5553 - accuracy: 0.7344\n",
      "Epoch 00039: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.5048 - accuracy: 0.7645 - val_loss: 0.4327 - val_accuracy: 0.9344\n",
      "Epoch 40/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5431 - accuracy: 0.6406\n",
      "Epoch 00040: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.4978 - accuracy: 0.7645 - val_loss: 0.4259 - val_accuracy: 0.9344\n",
      "Epoch 41/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5232 - accuracy: 0.7188\n",
      "Epoch 00041: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4885 - accuracy: 0.7521 - val_loss: 0.4190 - val_accuracy: 0.9344\n",
      "Epoch 42/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4140 - accuracy: 0.8281\n",
      "Epoch 00042: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4684 - accuracy: 0.7645 - val_loss: 0.4121 - val_accuracy: 0.9344\n",
      "Epoch 43/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4399 - accuracy: 0.7969\n",
      "Epoch 00043: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4697 - accuracy: 0.7893 - val_loss: 0.4051 - val_accuracy: 0.9344\n",
      "Epoch 44/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5075 - accuracy: 0.7188\n",
      "Epoch 00044: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.4727 - accuracy: 0.7521 - val_loss: 0.3988 - val_accuracy: 0.9344\n",
      "Epoch 45/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4743 - accuracy: 0.7812\n",
      "Epoch 00045: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4693 - accuracy: 0.7975 - val_loss: 0.3918 - val_accuracy: 0.9344\n",
      "Epoch 46/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5292 - accuracy: 0.7812\n",
      "Epoch 00046: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.4658 - accuracy: 0.8099 - val_loss: 0.3853 - val_accuracy: 0.9344\n",
      "Epoch 47/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4184 - accuracy: 0.8438\n",
      "Epoch 00047: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.4496 - accuracy: 0.7975 - val_loss: 0.3786 - val_accuracy: 0.9344\n",
      "Epoch 48/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5333 - accuracy: 0.7500\n",
      "Epoch 00048: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.4507 - accuracy: 0.8306 - val_loss: 0.3716 - val_accuracy: 0.9344\n",
      "Epoch 49/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4165 - accuracy: 0.8125\n",
      "Epoch 00049: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.4220 - accuracy: 0.8223 - val_loss: 0.3641 - val_accuracy: 0.9344\n",
      "Epoch 50/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4287 - accuracy: 0.7969\n",
      "Epoch 00050: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4528 - accuracy: 0.7769 - val_loss: 0.3574 - val_accuracy: 0.9344\n",
      "Epoch 51/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4463 - accuracy: 0.7500\n",
      "Epoch 00051: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.4764 - accuracy: 0.7397 - val_loss: 0.3516 - val_accuracy: 0.9344\n",
      "Epoch 52/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3898 - accuracy: 0.8281\n",
      "Epoch 00052: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.4394 - accuracy: 0.7851 - val_loss: 0.3455 - val_accuracy: 0.9344\n",
      "Epoch 53/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4700 - accuracy: 0.7812\n",
      "Epoch 00053: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4296 - accuracy: 0.7975 - val_loss: 0.3399 - val_accuracy: 0.9344\n",
      "Epoch 54/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4740 - accuracy: 0.7500\n",
      "Epoch 00054: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.4171 - accuracy: 0.8306 - val_loss: 0.3341 - val_accuracy: 0.9344\n",
      "Epoch 55/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4852 - accuracy: 0.7500\n",
      "Epoch 00055: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4188 - accuracy: 0.8099 - val_loss: 0.3270 - val_accuracy: 0.9344\n",
      "Epoch 56/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3870 - accuracy: 0.8438\n",
      "Epoch 00056: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.4221 - accuracy: 0.8140 - val_loss: 0.3203 - val_accuracy: 0.9344\n",
      "Epoch 57/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4051 - accuracy: 0.8594\n",
      "Epoch 00057: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3919 - accuracy: 0.8430 - val_loss: 0.3133 - val_accuracy: 0.9344\n",
      "Epoch 58/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4439 - accuracy: 0.8125\n",
      "Epoch 00058: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3935 - accuracy: 0.8182 - val_loss: 0.3067 - val_accuracy: 0.9344\n",
      "Epoch 59/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3535 - accuracy: 0.8125\n",
      "Epoch 00059: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.4023 - accuracy: 0.8058 - val_loss: 0.3000 - val_accuracy: 0.9344\n",
      "Epoch 60/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3910 - accuracy: 0.8438\n",
      "Epoch 00060: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3643 - accuracy: 0.8595 - val_loss: 0.2935 - val_accuracy: 0.9344\n",
      "Epoch 61/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3462 - accuracy: 0.8438\n",
      "Epoch 00061: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3569 - accuracy: 0.8636 - val_loss: 0.2853 - val_accuracy: 0.9344\n",
      "Epoch 62/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3657 - accuracy: 0.8594\n",
      "Epoch 00062: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3660 - accuracy: 0.8471 - val_loss: 0.2761 - val_accuracy: 0.9344\n",
      "Epoch 63/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3741 - accuracy: 0.8594\n",
      "Epoch 00063: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3611 - accuracy: 0.8636 - val_loss: 0.2663 - val_accuracy: 0.9344\n",
      "Epoch 64/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3533 - accuracy: 0.8750\n",
      "Epoch 00064: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3441 - accuracy: 0.8802 - val_loss: 0.2560 - val_accuracy: 0.9344\n",
      "Epoch 65/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3911 - accuracy: 0.8438\n",
      "Epoch 00065: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3603 - accuracy: 0.8388 - val_loss: 0.2449 - val_accuracy: 0.9344\n",
      "Epoch 66/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2406 - accuracy: 0.9375\n",
      "Epoch 00066: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3480 - accuracy: 0.8430 - val_loss: 0.2351 - val_accuracy: 0.9344\n",
      "Epoch 67/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3599 - accuracy: 0.8906\n",
      "Epoch 00067: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3542 - accuracy: 0.8678 - val_loss: 0.2268 - val_accuracy: 0.9344\n",
      "Epoch 68/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3600 - accuracy: 0.8125\n",
      "Epoch 00068: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3685 - accuracy: 0.8223 - val_loss: 0.2199 - val_accuracy: 0.9344\n",
      "Epoch 69/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3452 - accuracy: 0.8281\n",
      "Epoch 00069: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3602 - accuracy: 0.8512 - val_loss: 0.2139 - val_accuracy: 0.9344\n",
      "Epoch 70/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3319 - accuracy: 0.8750\n",
      "Epoch 00070: val_accuracy did not improve from 0.93443\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3195 - accuracy: 0.8802 - val_loss: 0.2087 - val_accuracy: 0.9344\n",
      "Epoch 71/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3487 - accuracy: 0.8281\n",
      "Epoch 00071: val_accuracy improved from 0.93443 to 0.95082, saving model to .\\modelo_mlp_ex3_2.hdf5\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3147 - accuracy: 0.8843 - val_loss: 0.2038 - val_accuracy: 0.9508\n",
      "Epoch 72/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2887 - accuracy: 0.8906\n",
      "Epoch 00072: val_accuracy did not improve from 0.95082\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3219 - accuracy: 0.8678 - val_loss: 0.1976 - val_accuracy: 0.9508\n",
      "Epoch 73/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2444 - accuracy: 0.9219\n",
      "Epoch 00073: val_accuracy did not improve from 0.95082\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2721 - accuracy: 0.9174 - val_loss: 0.1907 - val_accuracy: 0.9508\n",
      "Epoch 74/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3544 - accuracy: 0.8750\n",
      "Epoch 00074: val_accuracy did not improve from 0.95082\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3127 - accuracy: 0.8926 - val_loss: 0.1840 - val_accuracy: 0.9508\n",
      "Epoch 75/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2485 - accuracy: 0.9219\n",
      "Epoch 00075: val_accuracy did not improve from 0.95082\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.2720 - accuracy: 0.9091 - val_loss: 0.1777 - val_accuracy: 0.9508\n",
      "Epoch 76/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3227 - accuracy: 0.8906\n",
      "Epoch 00076: val_accuracy did not improve from 0.95082\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.2772 - accuracy: 0.9174 - val_loss: 0.1719 - val_accuracy: 0.9508\n",
      "Epoch 77/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2830 - accuracy: 0.8594\n",
      "Epoch 00077: val_accuracy did not improve from 0.95082\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3052 - accuracy: 0.8884 - val_loss: 0.1665 - val_accuracy: 0.9508\n",
      "Epoch 78/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2347 - accuracy: 0.9219\n",
      "Epoch 00078: val_accuracy improved from 0.95082 to 0.96721, saving model to .\\modelo_mlp_ex3_2.hdf5\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2720 - accuracy: 0.8967 - val_loss: 0.1612 - val_accuracy: 0.9672\n",
      "Epoch 79/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2907 - accuracy: 0.8906\n",
      "Epoch 00079: val_accuracy did not improve from 0.96721\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2737 - accuracy: 0.8926 - val_loss: 0.1563 - val_accuracy: 0.9672\n",
      "Epoch 80/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2055 - accuracy: 0.9688\n",
      "Epoch 00080: val_accuracy did not improve from 0.96721\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.2495 - accuracy: 0.9050 - val_loss: 0.1516 - val_accuracy: 0.9508\n",
      "Epoch 81/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1970 - accuracy: 0.9375\n",
      "Epoch 00081: val_accuracy did not improve from 0.96721\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2391 - accuracy: 0.9174 - val_loss: 0.1474 - val_accuracy: 0.9508\n",
      "Epoch 82/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2474 - accuracy: 0.9219\n",
      "Epoch 00082: val_accuracy did not improve from 0.96721\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.2466 - accuracy: 0.9091 - val_loss: 0.1436 - val_accuracy: 0.9508\n",
      "Epoch 83/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2447 - accuracy: 0.9375\n",
      "Epoch 00083: val_accuracy did not improve from 0.96721\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2539 - accuracy: 0.8967 - val_loss: 0.1400 - val_accuracy: 0.9508\n",
      "Epoch 84/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1933 - accuracy: 0.9219\n",
      "Epoch 00084: val_accuracy did not improve from 0.96721\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2469 - accuracy: 0.9132 - val_loss: 0.1363 - val_accuracy: 0.9672\n",
      "Epoch 85/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2397 - accuracy: 0.9531\n",
      "Epoch 00085: val_accuracy did not improve from 0.96721\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2551 - accuracy: 0.9132 - val_loss: 0.1323 - val_accuracy: 0.9508\n",
      "Epoch 86/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1953 - accuracy: 0.9375\n",
      "Epoch 00086: val_accuracy did not improve from 0.96721\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2465 - accuracy: 0.9050 - val_loss: 0.1289 - val_accuracy: 0.9508\n",
      "Epoch 87/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2391 - accuracy: 0.8906\n",
      "Epoch 00087: val_accuracy did not improve from 0.96721\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2286 - accuracy: 0.9174 - val_loss: 0.1257 - val_accuracy: 0.9672\n",
      "Epoch 88/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1864 - accuracy: 0.9375\n",
      "Epoch 00088: val_accuracy did not improve from 0.96721\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2164 - accuracy: 0.9091 - val_loss: 0.1225 - val_accuracy: 0.9672\n",
      "Epoch 89/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1811 - accuracy: 0.9688\n",
      "Epoch 00089: val_accuracy did not improve from 0.96721\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1969 - accuracy: 0.9421 - val_loss: 0.1196 - val_accuracy: 0.9672\n",
      "Epoch 90/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2245 - accuracy: 0.9062\n",
      "Epoch 00090: val_accuracy did not improve from 0.96721\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2085 - accuracy: 0.9132 - val_loss: 0.1170 - val_accuracy: 0.9672\n",
      "Epoch 91/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2332 - accuracy: 0.9062\n",
      "Epoch 00091: val_accuracy did not improve from 0.96721\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1940 - accuracy: 0.9256 - val_loss: 0.1138 - val_accuracy: 0.9672\n",
      "Epoch 92/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2727 - accuracy: 0.8594\n",
      "Epoch 00092: val_accuracy did not improve from 0.96721\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2098 - accuracy: 0.9174 - val_loss: 0.1107 - val_accuracy: 0.9672\n",
      "Epoch 93/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1915 - accuracy: 0.9375\n",
      "Epoch 00093: val_accuracy improved from 0.96721 to 0.98361, saving model to .\\modelo_mlp_ex3_2.hdf5\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.1752 - accuracy: 0.9504 - val_loss: 0.1075 - val_accuracy: 0.9836\n",
      "Epoch 94/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1677 - accuracy: 0.9219\n",
      "Epoch 00094: val_accuracy did not improve from 0.98361\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1971 - accuracy: 0.9339 - val_loss: 0.1044 - val_accuracy: 0.9836\n",
      "Epoch 95/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2339 - accuracy: 0.8750\n",
      "Epoch 00095: val_accuracy did not improve from 0.98361\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2058 - accuracy: 0.9339 - val_loss: 0.1016 - val_accuracy: 0.9836\n",
      "Epoch 96/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1585 - accuracy: 0.9375\n",
      "Epoch 00096: val_accuracy did not improve from 0.98361\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1844 - accuracy: 0.9298 - val_loss: 0.0991 - val_accuracy: 0.9672\n",
      "Epoch 97/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1859 - accuracy: 0.9375\n",
      "Epoch 00097: val_accuracy did not improve from 0.98361\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1852 - accuracy: 0.9421 - val_loss: 0.0962 - val_accuracy: 0.9672\n",
      "Epoch 98/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1323 - accuracy: 0.9844\n",
      "Epoch 00098: val_accuracy did not improve from 0.98361\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1664 - accuracy: 0.9421 - val_loss: 0.0936 - val_accuracy: 0.9672\n",
      "Epoch 99/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1928 - accuracy: 0.9219\n",
      "Epoch 00099: val_accuracy did not improve from 0.98361\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2018 - accuracy: 0.9174 - val_loss: 0.0911 - val_accuracy: 0.9836\n",
      "Epoch 100/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1284 - accuracy: 0.9688\n",
      "Epoch 00100: val_accuracy did not improve from 0.98361\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1953 - accuracy: 0.9174 - val_loss: 0.0885 - val_accuracy: 0.9836\n",
      "Epoch 101/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1386 - accuracy: 0.9688\n",
      "Epoch 00101: val_accuracy did not improve from 0.98361\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1836 - accuracy: 0.9380 - val_loss: 0.0862 - val_accuracy: 0.9672\n",
      "Epoch 102/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1791 - accuracy: 0.9375\n",
      "Epoch 00102: val_accuracy did not improve from 0.98361\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1740 - accuracy: 0.9298 - val_loss: 0.0839 - val_accuracy: 0.9672\n",
      "Epoch 103/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1695 - accuracy: 0.9531\n",
      "Epoch 00103: val_accuracy did not improve from 0.98361\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1914 - accuracy: 0.9380 - val_loss: 0.0819 - val_accuracy: 0.9836\n",
      "Epoch 104/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2286 - accuracy: 0.8906\n",
      "Epoch 00104: val_accuracy did not improve from 0.98361\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1860 - accuracy: 0.9215 - val_loss: 0.0798 - val_accuracy: 0.9836\n",
      "Epoch 105/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1275 - accuracy: 0.9375\n",
      "Epoch 00105: val_accuracy did not improve from 0.98361\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1813 - accuracy: 0.9339 - val_loss: 0.0777 - val_accuracy: 0.9836\n",
      "Epoch 106/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1560 - accuracy: 0.9375\n",
      "Epoch 00106: val_accuracy did not improve from 0.98361\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1444 - accuracy: 0.9463 - val_loss: 0.0758 - val_accuracy: 0.9836\n",
      "Epoch 107/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1522 - accuracy: 0.9531\n",
      "Epoch 00107: val_accuracy did not improve from 0.98361\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1826 - accuracy: 0.9380 - val_loss: 0.0742 - val_accuracy: 0.9836\n",
      "Epoch 108/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1583 - accuracy: 0.9688\n",
      "Epoch 00108: val_accuracy did not improve from 0.98361\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1501 - accuracy: 0.9752 - val_loss: 0.0725 - val_accuracy: 0.9836\n",
      "Epoch 109/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1444 - accuracy: 0.9531\n",
      "Epoch 00109: val_accuracy did not improve from 0.98361\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1528 - accuracy: 0.9545 - val_loss: 0.0707 - val_accuracy: 0.9836\n",
      "Epoch 110/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1331 - accuracy: 0.9688\n",
      "Epoch 00110: val_accuracy did not improve from 0.98361\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1347 - accuracy: 0.9752 - val_loss: 0.0686 - val_accuracy: 0.9836\n",
      "Epoch 111/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1214 - accuracy: 0.9531\n",
      "Epoch 00111: val_accuracy did not improve from 0.98361\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1269 - accuracy: 0.9463 - val_loss: 0.0667 - val_accuracy: 0.9836\n",
      "Epoch 112/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0758 - accuracy: 0.9844\n",
      "Epoch 00112: val_accuracy improved from 0.98361 to 1.00000, saving model to .\\modelo_mlp_ex3_2.hdf5\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.1326 - accuracy: 0.9628 - val_loss: 0.0648 - val_accuracy: 1.0000\n",
      "Epoch 113/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1213 - accuracy: 0.9531\n",
      "Epoch 00113: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1611 - accuracy: 0.9463 - val_loss: 0.0633 - val_accuracy: 1.0000\n",
      "Epoch 114/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1491 - accuracy: 0.9688\n",
      "Epoch 00114: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1406 - accuracy: 0.9628 - val_loss: 0.0620 - val_accuracy: 1.0000\n",
      "Epoch 115/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1320 - accuracy: 0.9531\n",
      "Epoch 00115: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1249 - accuracy: 0.9587 - val_loss: 0.0602 - val_accuracy: 1.0000\n",
      "Epoch 116/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1290 - accuracy: 0.9844\n",
      "Epoch 00116: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1282 - accuracy: 0.9669 - val_loss: 0.0584 - val_accuracy: 1.0000\n",
      "Epoch 117/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1283 - accuracy: 0.9375\n",
      "Epoch 00117: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1501 - accuracy: 0.9421 - val_loss: 0.0568 - val_accuracy: 1.0000\n",
      "Epoch 118/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0737 - accuracy: 1.0000\n",
      "Epoch 00118: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1126 - accuracy: 0.9587 - val_loss: 0.0550 - val_accuracy: 1.0000\n",
      "Epoch 119/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1561 - accuracy: 0.9531\n",
      "Epoch 00119: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1648 - accuracy: 0.9380 - val_loss: 0.0536 - val_accuracy: 1.0000\n",
      "Epoch 120/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1190 - accuracy: 0.9531\n",
      "Epoch 00120: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1050 - accuracy: 0.9628 - val_loss: 0.0521 - val_accuracy: 1.0000\n",
      "Epoch 121/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1743 - accuracy: 0.9531\n",
      "Epoch 00121: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1425 - accuracy: 0.9545 - val_loss: 0.0503 - val_accuracy: 1.0000\n",
      "Epoch 122/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1100 - accuracy: 0.9844\n",
      "Epoch 00122: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0912 - accuracy: 0.9835 - val_loss: 0.0484 - val_accuracy: 1.0000\n",
      "Epoch 123/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1312 - accuracy: 0.9688\n",
      "Epoch 00123: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1340 - accuracy: 0.9669 - val_loss: 0.0466 - val_accuracy: 1.0000\n",
      "Epoch 124/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1117 - accuracy: 0.9688\n",
      "Epoch 00124: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1367 - accuracy: 0.9545 - val_loss: 0.0448 - val_accuracy: 1.0000\n",
      "Epoch 125/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1370 - accuracy: 0.9688\n",
      "Epoch 00125: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1215 - accuracy: 0.9669 - val_loss: 0.0434 - val_accuracy: 1.0000\n",
      "Epoch 126/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0890 - accuracy: 0.9688\n",
      "Epoch 00126: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1209 - accuracy: 0.9545 - val_loss: 0.0420 - val_accuracy: 1.0000\n",
      "Epoch 127/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0825 - accuracy: 0.9844\n",
      "Epoch 00127: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0986 - accuracy: 0.9669 - val_loss: 0.0406 - val_accuracy: 1.0000\n",
      "Epoch 128/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1308 - accuracy: 0.9531\n",
      "Epoch 00128: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1078 - accuracy: 0.9711 - val_loss: 0.0394 - val_accuracy: 1.0000\n",
      "Epoch 129/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1151 - accuracy: 0.9844\n",
      "Epoch 00129: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1103 - accuracy: 0.9711 - val_loss: 0.0384 - val_accuracy: 1.0000\n",
      "Epoch 130/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0718 - accuracy: 1.0000\n",
      "Epoch 00130: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0982 - accuracy: 0.9752 - val_loss: 0.0377 - val_accuracy: 1.0000\n",
      "Epoch 131/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1509 - accuracy: 0.9219\n",
      "Epoch 00131: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1179 - accuracy: 0.9587 - val_loss: 0.0372 - val_accuracy: 1.0000\n",
      "Epoch 132/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1196 - accuracy: 0.9219\n",
      "Epoch 00132: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1343 - accuracy: 0.9504 - val_loss: 0.0367 - val_accuracy: 1.0000\n",
      "Epoch 133/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1050 - accuracy: 0.9844\n",
      "Epoch 00133: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0892 - accuracy: 0.9793 - val_loss: 0.0359 - val_accuracy: 1.0000\n",
      "Epoch 134/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1143 - accuracy: 1.0000\n",
      "Epoch 00134: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1032 - accuracy: 0.9835 - val_loss: 0.0352 - val_accuracy: 1.0000\n",
      "Epoch 135/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1756 - accuracy: 0.9375\n",
      "Epoch 00135: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.1184 - accuracy: 0.9587 - val_loss: 0.0348 - val_accuracy: 1.0000\n",
      "Epoch 136/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1626 - accuracy: 0.9375\n",
      "Epoch 00136: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1412 - accuracy: 0.9587 - val_loss: 0.0340 - val_accuracy: 1.0000\n",
      "Epoch 137/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0782 - accuracy: 0.9688\n",
      "Epoch 00137: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0968 - accuracy: 0.9669 - val_loss: 0.0331 - val_accuracy: 1.0000\n",
      "Epoch 138/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0920 - accuracy: 0.9844\n",
      "Epoch 00138: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1177 - accuracy: 0.9504 - val_loss: 0.0322 - val_accuracy: 1.0000\n",
      "Epoch 139/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0908 - accuracy: 0.9688\n",
      "Epoch 00139: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0848 - accuracy: 0.9669 - val_loss: 0.0316 - val_accuracy: 1.0000\n",
      "Epoch 140/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0550 - accuracy: 0.9844\n",
      "Epoch 00140: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0630 - accuracy: 0.9917 - val_loss: 0.0309 - val_accuracy: 1.0000\n",
      "Epoch 141/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1158 - accuracy: 0.9531\n",
      "Epoch 00141: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1028 - accuracy: 0.9752 - val_loss: 0.0303 - val_accuracy: 1.0000\n",
      "Epoch 142/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0671 - accuracy: 0.9688\n",
      "Epoch 00142: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.1167 - accuracy: 0.9504 - val_loss: 0.0291 - val_accuracy: 1.0000\n",
      "Epoch 143/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0861 - accuracy: 0.9531\n",
      "Epoch 00143: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0843 - accuracy: 0.9752 - val_loss: 0.0270 - val_accuracy: 1.0000\n",
      "Epoch 144/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1356 - accuracy: 0.9375\n",
      "Epoch 00144: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0977 - accuracy: 0.9669 - val_loss: 0.0254 - val_accuracy: 1.0000\n",
      "Epoch 145/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0752 - accuracy: 0.9844\n",
      "Epoch 00145: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0961 - accuracy: 0.9669 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "Epoch 146/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0780 - accuracy: 0.9844\n",
      "Epoch 00146: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0794 - accuracy: 0.9711 - val_loss: 0.0236 - val_accuracy: 1.0000\n",
      "Epoch 147/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1441 - accuracy: 0.9531\n",
      "Epoch 00147: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0912 - accuracy: 0.9752 - val_loss: 0.0231 - val_accuracy: 1.0000\n",
      "Epoch 148/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0528 - accuracy: 0.9844\n",
      "Epoch 00148: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0607 - accuracy: 0.9917 - val_loss: 0.0224 - val_accuracy: 1.0000\n",
      "Epoch 149/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0897 - accuracy: 0.9844\n",
      "Epoch 00149: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0799 - accuracy: 0.9793 - val_loss: 0.0216 - val_accuracy: 1.0000\n",
      "Epoch 150/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0890 - accuracy: 0.9688\n",
      "Epoch 00150: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0793 - accuracy: 0.9752 - val_loss: 0.0208 - val_accuracy: 1.0000\n",
      "Epoch 151/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0961 - accuracy: 0.9688\n",
      "Epoch 00151: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0898 - accuracy: 0.9711 - val_loss: 0.0199 - val_accuracy: 1.0000\n",
      "Epoch 152/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0686 - accuracy: 0.9844\n",
      "Epoch 00152: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0726 - accuracy: 0.9793 - val_loss: 0.0193 - val_accuracy: 1.0000\n",
      "Epoch 153/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0691 - accuracy: 0.9844\n",
      "Epoch 00153: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0775 - accuracy: 0.9793 - val_loss: 0.0185 - val_accuracy: 1.0000\n",
      "Epoch 154/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1078 - accuracy: 0.9688\n",
      "Epoch 00154: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0953 - accuracy: 0.9669 - val_loss: 0.0177 - val_accuracy: 1.0000\n",
      "Epoch 155/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0953 - accuracy: 0.9688\n",
      "Epoch 00155: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0714 - accuracy: 0.9752 - val_loss: 0.0169 - val_accuracy: 1.0000\n",
      "Epoch 156/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0535 - accuracy: 1.0000\n",
      "Epoch 00156: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0705 - accuracy: 0.9752 - val_loss: 0.0163 - val_accuracy: 1.0000\n",
      "Epoch 157/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0484 - accuracy: 0.9844\n",
      "Epoch 00157: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0747 - accuracy: 0.9711 - val_loss: 0.0157 - val_accuracy: 1.0000\n",
      "Epoch 158/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1391 - accuracy: 0.9375\n",
      "Epoch 00158: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.1073 - accuracy: 0.9628 - val_loss: 0.0150 - val_accuracy: 1.0000\n",
      "Epoch 159/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0213 - accuracy: 1.0000\n",
      "Epoch 00159: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0470 - accuracy: 0.9917 - val_loss: 0.0145 - val_accuracy: 1.0000\n",
      "Epoch 160/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0938 - accuracy: 0.9844\n",
      "Epoch 00160: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0842 - accuracy: 0.9752 - val_loss: 0.0142 - val_accuracy: 1.0000\n",
      "Epoch 161/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0704 - accuracy: 0.9844\n",
      "Epoch 00161: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0733 - accuracy: 0.9876 - val_loss: 0.0140 - val_accuracy: 1.0000\n",
      "Epoch 162/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0806 - accuracy: 1.0000\n",
      "Epoch 00162: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0646 - accuracy: 0.9876 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
      "Epoch 163/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0687 - accuracy: 0.9844\n",
      "Epoch 00163: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0702 - accuracy: 0.9793 - val_loss: 0.0132 - val_accuracy: 1.0000\n",
      "Epoch 164/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0341 - accuracy: 1.0000\n",
      "Epoch 00164: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0679 - accuracy: 0.9876 - val_loss: 0.0127 - val_accuracy: 1.0000\n",
      "Epoch 165/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0552 - accuracy: 1.0000\n",
      "Epoch 00165: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0593 - accuracy: 0.9917 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
      "Epoch 166/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0815 - accuracy: 0.9688\n",
      "Epoch 00166: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0602 - accuracy: 0.9793 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
      "Epoch 167/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0524 - accuracy: 0.9844\n",
      "Epoch 00167: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0778 - accuracy: 0.9752 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
      "Epoch 168/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0634 - accuracy: 0.9688\n",
      "Epoch 00168: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0584 - accuracy: 0.9917 - val_loss: 0.0116 - val_accuracy: 1.0000\n",
      "Epoch 169/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0543 - accuracy: 0.9688\n",
      "Epoch 00169: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0672 - accuracy: 0.9669 - val_loss: 0.0114 - val_accuracy: 1.0000\n",
      "Epoch 170/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0792 - accuracy: 0.9688\n",
      "Epoch 00170: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0587 - accuracy: 0.9835 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
      "Epoch 171/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0647 - accuracy: 0.9844\n",
      "Epoch 00171: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0728 - accuracy: 0.9752 - val_loss: 0.0110 - val_accuracy: 1.0000\n",
      "Epoch 172/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1123 - accuracy: 0.9375\n",
      "Epoch 00172: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0815 - accuracy: 0.9669 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
      "Epoch 173/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0386 - accuracy: 0.9844\n",
      "Epoch 00173: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0591 - accuracy: 0.9876 - val_loss: 0.0103 - val_accuracy: 1.0000\n",
      "Epoch 174/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0724 - accuracy: 0.9688\n",
      "Epoch 00174: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0964 - accuracy: 0.9628 - val_loss: 0.0098 - val_accuracy: 1.0000\n",
      "Epoch 175/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0412 - accuracy: 1.0000\n",
      "Epoch 00175: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0589 - accuracy: 0.9835 - val_loss: 0.0095 - val_accuracy: 1.0000\n",
      "Epoch 176/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0500 - accuracy: 0.9688\n",
      "Epoch 00176: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0559 - accuracy: 0.9793 - val_loss: 0.0093 - val_accuracy: 1.0000\n",
      "Epoch 177/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0599 - accuracy: 0.9844\n",
      "Epoch 00177: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0745 - accuracy: 0.9711 - val_loss: 0.0091 - val_accuracy: 1.0000\n",
      "Epoch 178/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0437 - accuracy: 1.0000\n",
      "Epoch 00178: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0476 - accuracy: 0.9917 - val_loss: 0.0090 - val_accuracy: 1.0000\n",
      "Epoch 179/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0796 - accuracy: 0.9688\n",
      "Epoch 00179: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0721 - accuracy: 0.9793 - val_loss: 0.0089 - val_accuracy: 1.0000\n",
      "Epoch 180/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0599 - accuracy: 0.9844\n",
      "Epoch 00180: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0742 - accuracy: 0.9917 - val_loss: 0.0087 - val_accuracy: 1.0000\n",
      "Epoch 181/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0526 - accuracy: 1.0000\n",
      "Epoch 00181: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0410 - accuracy: 0.9959 - val_loss: 0.0085 - val_accuracy: 1.0000\n",
      "Epoch 182/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0234 - accuracy: 1.0000\n",
      "Epoch 00182: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0487 - accuracy: 0.9876 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
      "Epoch 183/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0430 - accuracy: 1.0000\n",
      "Epoch 00183: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0640 - accuracy: 0.9876 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
      "Epoch 184/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1249 - accuracy: 0.9531\n",
      "Epoch 00184: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0861 - accuracy: 0.9669 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
      "Epoch 185/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0527 - accuracy: 0.9844\n",
      "Epoch 00185: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0477 - accuracy: 0.9876 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
      "Epoch 186/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0361 - accuracy: 1.0000\n",
      "Epoch 00186: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0476 - accuracy: 0.9959 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
      "Epoch 187/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0563 - accuracy: 1.0000\n",
      "Epoch 00187: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0388 - accuracy: 0.9959 - val_loss: 0.0080 - val_accuracy: 1.0000\n",
      "Epoch 188/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0313 - accuracy: 1.0000\n",
      "Epoch 00188: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0350 - accuracy: 0.9959 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
      "Epoch 189/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0464 - accuracy: 0.9844\n",
      "Epoch 00189: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0349 - accuracy: 0.9876 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
      "Epoch 190/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0220 - accuracy: 1.0000\n",
      "Epoch 00190: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0528 - accuracy: 0.9876 - val_loss: 0.0077 - val_accuracy: 1.0000\n",
      "Epoch 191/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1198 - accuracy: 0.9688\n",
      "Epoch 00191: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0545 - accuracy: 0.9917 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
      "Epoch 192/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0461 - accuracy: 1.0000\n",
      "Epoch 00192: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0668 - accuracy: 0.9835 - val_loss: 0.0073 - val_accuracy: 1.0000\n",
      "Epoch 193/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0892 - accuracy: 0.9688\n",
      "Epoch 00193: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0690 - accuracy: 0.9752 - val_loss: 0.0074 - val_accuracy: 1.0000\n",
      "Epoch 194/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0641 - accuracy: 0.9688\n",
      "Epoch 00194: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0451 - accuracy: 0.9835 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
      "Epoch 195/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1491 - accuracy: 0.9531\n",
      "Epoch 00195: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0862 - accuracy: 0.9752 - val_loss: 0.0067 - val_accuracy: 1.0000\n",
      "Epoch 196/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0567 - accuracy: 0.9688\n",
      "Epoch 00196: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0502 - accuracy: 0.9793 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
      "Epoch 197/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0777 - accuracy: 0.9844\n",
      "Epoch 00197: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0789 - accuracy: 0.9669 - val_loss: 0.0059 - val_accuracy: 1.0000\n",
      "Epoch 198/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0486 - accuracy: 0.9844\n",
      "Epoch 00198: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0473 - accuracy: 0.9876 - val_loss: 0.0057 - val_accuracy: 1.0000\n",
      "Epoch 199/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0348 - accuracy: 0.9844\n",
      "Epoch 00199: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0412 - accuracy: 0.9793 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
      "Epoch 200/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0338 - accuracy: 0.9844\n",
      "Epoch 00200: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0470 - accuracy: 0.9835 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
      "Epoch 201/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0547 - accuracy: 0.9844\n",
      "Epoch 00201: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0633 - accuracy: 0.9876 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
      "Epoch 202/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0467 - accuracy: 1.0000\n",
      "Epoch 00202: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0686 - accuracy: 0.9752 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
      "Epoch 203/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0258 - accuracy: 0.9844\n",
      "Epoch 00203: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0356 - accuracy: 0.9917 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
      "Epoch 204/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0583 - accuracy: 0.9844\n",
      "Epoch 00204: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0489 - accuracy: 0.9917 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
      "Epoch 205/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0448 - accuracy: 1.0000\n",
      "Epoch 00205: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0403 - accuracy: 0.9917 - val_loss: 0.0049 - val_accuracy: 1.0000\n",
      "Epoch 206/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1038 - accuracy: 0.9688\n",
      "Epoch 00206: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0627 - accuracy: 0.9793 - val_loss: 0.0048 - val_accuracy: 1.0000\n",
      "Epoch 207/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0696 - accuracy: 0.9688\n",
      "Epoch 00207: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0582 - accuracy: 0.9876 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
      "Epoch 208/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0340 - accuracy: 1.0000\n",
      "Epoch 00208: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0307 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
      "Epoch 209/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0423 - accuracy: 0.9844\n",
      "Epoch 00209: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0315 - accuracy: 0.9959 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
      "Epoch 210/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0179 - accuracy: 1.0000\n",
      "Epoch 00210: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0234 - accuracy: 0.9959 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
      "Epoch 211/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0539 - accuracy: 0.9844\n",
      "Epoch 00211: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0441 - accuracy: 0.9876 - val_loss: 0.0043 - val_accuracy: 1.0000\n",
      "Epoch 212/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0871 - accuracy: 0.9844\n",
      "Epoch 00212: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0550 - accuracy: 0.9876 - val_loss: 0.0043 - val_accuracy: 1.0000\n",
      "Epoch 213/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0312 - accuracy: 0.9844\n",
      "Epoch 00213: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0490 - accuracy: 0.9876 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
      "Epoch 214/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0685 - accuracy: 0.9844\n",
      "Epoch 00214: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0423 - accuracy: 0.9917 - val_loss: 0.0043 - val_accuracy: 1.0000\n",
      "Epoch 215/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1007 - accuracy: 0.9531\n",
      "Epoch 00215: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0496 - accuracy: 0.9835 - val_loss: 0.0043 - val_accuracy: 1.0000\n",
      "Epoch 216/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0268 - accuracy: 0.9844\n",
      "Epoch 00216: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0434 - accuracy: 0.9876 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
      "Epoch 217/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0613 - accuracy: 0.9688\n",
      "Epoch 00217: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0463 - accuracy: 0.9835 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
      "Epoch 218/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0632 - accuracy: 0.9844\n",
      "Epoch 00218: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0431 - accuracy: 0.9959 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "Epoch 219/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0381 - accuracy: 0.9844\n",
      "Epoch 00219: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0478 - accuracy: 0.9876 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
      "Epoch 220/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0399 - accuracy: 1.0000\n",
      "Epoch 00220: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0522 - accuracy: 0.9917 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
      "Epoch 221/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 00221: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0291 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 222/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0322 - accuracy: 1.0000\n",
      "Epoch 00222: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0407 - accuracy: 0.9835 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 223/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0401 - accuracy: 0.9844\n",
      "Epoch 00223: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0503 - accuracy: 0.9835 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 224/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0358 - accuracy: 1.0000\n",
      "Epoch 00224: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0288 - accuracy: 0.9959 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 225/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0480 - accuracy: 0.9844\n",
      "Epoch 00225: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0454 - accuracy: 0.9876 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 226/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0379 - accuracy: 1.0000\n",
      "Epoch 00226: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0387 - accuracy: 0.9835 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 227/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0487 - accuracy: 0.9844\n",
      "Epoch 00227: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0254 - accuracy: 0.9959 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 228/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 00228: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 229/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0901 - accuracy: 0.9844\n",
      "Epoch 00229: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0431 - accuracy: 0.9917 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 230/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0782 - accuracy: 0.9688\n",
      "Epoch 00230: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0366 - accuracy: 0.9917 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 231/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0212 - accuracy: 0.9844\n",
      "Epoch 00231: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0431 - accuracy: 0.9917 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 232/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0704 - accuracy: 0.9531\n",
      "Epoch 00232: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0587 - accuracy: 0.9711 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 233/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0537 - accuracy: 0.9844\n",
      "Epoch 00233: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0424 - accuracy: 0.9917 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 234/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0208 - accuracy: 1.0000\n",
      "Epoch 00234: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0364 - accuracy: 0.9876 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 235/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0444 - accuracy: 0.9844\n",
      "Epoch 00235: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0345 - accuracy: 0.9959 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 236/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0538 - accuracy: 0.9688\n",
      "Epoch 00236: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0317 - accuracy: 0.9917 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 237/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0225 - accuracy: 1.0000\n",
      "Epoch 00237: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0231 - accuracy: 0.9917 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 238/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0316 - accuracy: 1.0000\n",
      "Epoch 00238: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0306 - accuracy: 0.9959 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 239/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0285 - accuracy: 1.0000\n",
      "Epoch 00239: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0273 - accuracy: 0.9959 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 240/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1086 - accuracy: 0.9844\n",
      "Epoch 00240: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0480 - accuracy: 0.9917 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 241/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0360 - accuracy: 1.0000\n",
      "Epoch 00241: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0296 - accuracy: 0.9917 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 242/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0642 - accuracy: 0.9844\n",
      "Epoch 00242: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0428 - accuracy: 0.9917 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 243/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0187 - accuracy: 0.9844\n",
      "Epoch 00243: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0301 - accuracy: 0.9959 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 244/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0280 - accuracy: 1.0000\n",
      "Epoch 00244: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0301 - accuracy: 0.9959 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 245/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0200 - accuracy: 1.0000\n",
      "Epoch 00245: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0269 - accuracy: 0.9959 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 246/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0299 - accuracy: 1.0000\n",
      "Epoch 00246: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0235 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 247/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0294 - accuracy: 0.9844\n",
      "Epoch 00247: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0324 - accuracy: 0.9876 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 248/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 00248: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0289 - accuracy: 0.9917 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 249/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0169 - accuracy: 1.0000\n",
      "Epoch 00249: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0315 - accuracy: 0.9917 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 250/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0178 - accuracy: 1.0000\n",
      "Epoch 00250: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0395 - accuracy: 0.9876 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 251/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0228 - accuracy: 1.0000\n",
      "Epoch 00251: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0263 - accuracy: 0.9959 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 252/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0656 - accuracy: 0.9844\n",
      "Epoch 00252: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0352 - accuracy: 0.9959 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 253/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 00253: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0275 - accuracy: 0.9917 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 254/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0363 - accuracy: 0.9844\n",
      "Epoch 00254: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0299 - accuracy: 0.9876 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 255/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 00255: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 256/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0398 - accuracy: 1.0000\n",
      "Epoch 00256: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0236 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 257/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0293 - accuracy: 1.0000\n",
      "Epoch 00257: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0222 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 258/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0359 - accuracy: 1.0000\n",
      "Epoch 00258: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0312 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 259/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0456 - accuracy: 0.9688\n",
      "Epoch 00259: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0264 - accuracy: 0.9917 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 260/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0312 - accuracy: 0.9844\n",
      "Epoch 00260: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0312 - accuracy: 0.9876 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 261/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0439 - accuracy: 0.9688\n",
      "Epoch 00261: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0301 - accuracy: 0.9876 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 262/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0125 - accuracy: 1.0000\n",
      "Epoch 00262: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0177 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 263/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0233 - accuracy: 0.9844\n",
      "Epoch 00263: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0354 - accuracy: 0.9835 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 264/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0197 - accuracy: 0.9844\n",
      "Epoch 00264: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0277 - accuracy: 0.9876 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 265/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0165 - accuracy: 1.0000\n",
      "Epoch 00265: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 266/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0338 - accuracy: 0.9844\n",
      "Epoch 00266: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0360 - accuracy: 0.9876 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 267/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 00267: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0326 - accuracy: 0.9876 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 268/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 00268: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0229 - accuracy: 0.9959 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 269/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0439 - accuracy: 0.9688\n",
      "Epoch 00269: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0374 - accuracy: 0.9835 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 270/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0585 - accuracy: 0.9844\n",
      "Epoch 00270: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0339 - accuracy: 0.9835 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 271/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0231 - accuracy: 1.0000\n",
      "Epoch 00271: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0163 - accuracy: 0.9917 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 272/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 00272: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0140 - accuracy: 0.9959 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 273/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 00273: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0199 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 274/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0221 - accuracy: 1.0000\n",
      "Epoch 00274: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0319 - accuracy: 0.9917 - val_loss: 9.8934e-04 - val_accuracy: 1.0000\n",
      "Epoch 275/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0442 - accuracy: 0.9688\n",
      "Epoch 00275: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0258 - accuracy: 0.9876 - val_loss: 9.7476e-04 - val_accuracy: 1.0000\n",
      "Epoch 276/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0171 - accuracy: 1.0000\n",
      "Epoch 00276: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0264 - accuracy: 0.9959 - val_loss: 9.5417e-04 - val_accuracy: 1.0000\n",
      "Epoch 277/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 00277: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0212 - accuracy: 0.9917 - val_loss: 9.3466e-04 - val_accuracy: 1.0000\n",
      "Epoch 278/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 00278: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0178 - accuracy: 0.9917 - val_loss: 9.1638e-04 - val_accuracy: 1.0000\n",
      "Epoch 279/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0358 - accuracy: 0.9844\n",
      "Epoch 00279: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0220 - accuracy: 0.9959 - val_loss: 8.9995e-04 - val_accuracy: 1.0000\n",
      "Epoch 280/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0265 - accuracy: 1.0000\n",
      "Epoch 00280: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0201 - accuracy: 0.9959 - val_loss: 8.8146e-04 - val_accuracy: 1.0000\n",
      "Epoch 281/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0154 - accuracy: 1.0000\n",
      "Epoch 00281: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0180 - accuracy: 1.0000 - val_loss: 8.6819e-04 - val_accuracy: 1.0000\n",
      "Epoch 282/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0179 - accuracy: 1.0000\n",
      "Epoch 00282: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0242 - accuracy: 0.9876 - val_loss: 8.5364e-04 - val_accuracy: 1.0000\n",
      "Epoch 283/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 00283: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0178 - accuracy: 0.9917 - val_loss: 8.3746e-04 - val_accuracy: 1.0000\n",
      "Epoch 284/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0187 - accuracy: 1.0000\n",
      "Epoch 00284: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 8.2308e-04 - val_accuracy: 1.0000\n",
      "Epoch 285/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0206 - accuracy: 1.0000\n",
      "Epoch 00285: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0230 - accuracy: 0.9917 - val_loss: 8.1290e-04 - val_accuracy: 1.0000\n",
      "Epoch 286/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0288 - accuracy: 0.9844\n",
      "Epoch 00286: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0228 - accuracy: 0.9917 - val_loss: 8.0454e-04 - val_accuracy: 1.0000\n",
      "Epoch 287/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0472 - accuracy: 0.9688\n",
      "Epoch 00287: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0270 - accuracy: 0.9835 - val_loss: 7.9148e-04 - val_accuracy: 1.0000\n",
      "Epoch 288/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0368 - accuracy: 0.9844\n",
      "Epoch 00288: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0274 - accuracy: 0.9876 - val_loss: 7.8874e-04 - val_accuracy: 1.0000\n",
      "Epoch 289/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0312 - accuracy: 0.9844\n",
      "Epoch 00289: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0330 - accuracy: 0.9835 - val_loss: 7.9184e-04 - val_accuracy: 1.0000\n",
      "Epoch 290/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 00290: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0321 - accuracy: 0.9917 - val_loss: 7.9920e-04 - val_accuracy: 1.0000\n",
      "Epoch 291/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0415 - accuracy: 0.9688\n",
      "Epoch 00291: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0312 - accuracy: 0.9835 - val_loss: 8.0457e-04 - val_accuracy: 1.0000\n",
      "Epoch 292/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0211 - accuracy: 0.9844\n",
      "Epoch 00292: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0143 - accuracy: 0.9959 - val_loss: 7.8537e-04 - val_accuracy: 1.0000\n",
      "Epoch 293/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 00293: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0216 - accuracy: 0.9876 - val_loss: 7.4375e-04 - val_accuracy: 1.0000\n",
      "Epoch 294/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 00294: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 7.0098e-04 - val_accuracy: 1.0000\n",
      "Epoch 295/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 00295: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 6.6789e-04 - val_accuracy: 1.0000\n",
      "Epoch 296/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 00296: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0238 - accuracy: 0.9917 - val_loss: 6.4463e-04 - val_accuracy: 1.0000\n",
      "Epoch 297/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0390 - accuracy: 1.0000\n",
      "Epoch 00297: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0233 - accuracy: 1.0000 - val_loss: 6.1153e-04 - val_accuracy: 1.0000\n",
      "Epoch 298/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0378 - accuracy: 0.9688\n",
      "Epoch 00298: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0232 - accuracy: 0.9876 - val_loss: 5.8801e-04 - val_accuracy: 1.0000\n",
      "Epoch 299/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0550 - accuracy: 0.9688\n",
      "Epoch 00299: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0192 - accuracy: 0.9917 - val_loss: 5.7259e-04 - val_accuracy: 1.0000\n",
      "Epoch 300/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.0226 - accuracy: 1.0000\n",
      "Epoch 00300: val_accuracy did not improve from 1.00000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0152 - accuracy: 0.9959 - val_loss: 5.6037e-04 - val_accuracy: 1.0000\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\r\n",
    "checkpointer = ModelCheckpoint(filepath='./modelo_mlp_ex3_2.hdf5', verbose=1,  save_best_only=True, monitor='val_accuracy')\r\n",
    "\r\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=300, batch_size=64, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gráfico comparativo de Acurácia e Perda no treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando o melhor modelo salvo e apresentando a acurácia do Treino e do Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"./modelo_mlp_ex3_2.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0621 - accuracy: 0.9959\n",
      "\n",
      "Acuracia do Treino: 99.59%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_train, y_train)\r\n",
    "print()\r\n",
    "print(f\"Acuracia do Treino: {round(scores[1]*100,2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0648 - accuracy: 1.0000\n",
      "\n",
      "Acuracia da Validação: 100.0%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test)\r\n",
    "print()\r\n",
    "print(f\"Acuracia da Validação: {round(scores[1]*100,2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conferindo o modelo final com uma Validação Cruzada (Cross Validation) usando 10 amostras. Neste caso vamos usar toda a base de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\r\n",
    "    model = Sequential()\r\n",
    "    model.add(Dense(13, input_dim=num_features, activation='relu'))\r\n",
    "    model.add(Dropout(0.2))\r\n",
    "    model.add(Dense(7, activation='relu'))\r\n",
    "    model.add(Dropout(0.2))\r\n",
    "    model.add(Dense(1 , activation='sigmoid'))\r\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
    "    \r\n",
    "    return model\r\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000012FA93E14C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000012FB19C7670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000012FB97C65E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000012FB0CDFD30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000012FB98FB0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000012FAF44C4C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000012FB876A3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000012FA93F05E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000012FB8774160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000012FB1569D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "Accuracy : 1.00 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(action='ignore')\r\n",
    "\r\n",
    "estimator = KerasClassifier(build_fn=create_model, epochs=300, batch_size=64, verbose=0)\r\n",
    "scores = cross_val_score(estimator, X, y, cv=10, n_jobs=1)\r\n",
    "print()\r\n",
    "print(\"Accuracy : {:0.2f} (+/- {:0.2f})\".format(scores.mean(), scores.std()))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c24c9dacf042e5cf8b743bae11b2cef3a95983df3bc5153773d9ffef1d5207d2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('tf-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}